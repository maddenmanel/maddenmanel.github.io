<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-02-20T18:18:36+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Madden’s Blog</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><author><name>Madden Zhang</name></author><entry><title type="html">Distribute Design Clickhouse</title><link href="http://localhost:4000/blog/distribute-design-clickhouse/" rel="alternate" type="text/html" title="Distribute Design Clickhouse" /><published>2025-02-20T00:00:00+08:00</published><updated>2025-02-20T00:00:00+08:00</updated><id>http://localhost:4000/blog/distribute-design-clickhouse</id><content type="html" xml:base="http://localhost:4000/blog/distribute-design-clickhouse/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>ClickHouse, an open-source columnar database management system (DBMS), is designed for online analytical processing (OLAP). It provides real-time data analysis with extremely fast query performance, even on massive datasets. Originally developed by Yandex, ClickHouse has become a popular solution for analytics-heavy use cases in large-scale environments, supporting high throughput and low-latency queries.</p>

<p>ClickHouse’s architecture is engineered for distributed processing, offering both horizontal scalability and fault tolerance. This article delves into ClickHouse’s architecture and its core features, including data distribution, replication, and fault tolerance mechanisms.</p>

<h2 id="key-terminologies">Key Terminologies</h2>

<table>
  <thead>
    <tr>
      <th>Term</th>
      <th>Explanation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Shard</strong></td>
      <td>A physical subset of data that is stored on a specific set of nodes.</td>
    </tr>
    <tr>
      <td><strong>Replica</strong></td>
      <td>A copy of a shard’s data for fault tolerance and high availability.</td>
    </tr>
    <tr>
      <td><strong>Table</strong></td>
      <td>A collection of data organized into columns.</td>
    </tr>
    <tr>
      <td><strong>MergeTree</strong></td>
      <td>A special type of table that supports fast querying and efficient storage.</td>
    </tr>
    <tr>
      <td><strong>Partition</strong></td>
      <td>A logical division of data within a table, often mapped to a specific time period.</td>
    </tr>
    <tr>
      <td><strong>Distributed Table</strong></td>
      <td>A table that abstracts data stored on different nodes in a cluster.</td>
    </tr>
    <tr>
      <td><strong>Replica Synchronization</strong></td>
      <td>The process by which replicas stay in sync with their leader shard.</td>
    </tr>
    <tr>
      <td><strong>Materialized View</strong></td>
      <td>A precomputed view that stores the result of a query to optimize query performance.</td>
    </tr>
  </tbody>
</table>

<h2 id="clickhouses-storage-mechanism">ClickHouse’s Storage Mechanism</h2>

<h3 id="columnar-storage">Columnar Storage</h3>

<p>ClickHouse’s storage engine is optimized for OLAP workloads. It stores data in columnar format, which is ideal for analytical queries that often only access a few columns at a time. This section explores the data distribution, partitioning, and table storage mechanisms in ClickHouse.</p>

<p>What is the the difference between row format and columnar format?</p>

<div class="mermaid">
graph TD
    subgraph Row_Store[Row Storage]
        direction LR
        A1[User ID: 1] --&gt; B1[Name: Zhang San] --&gt; C1[Age: 25] --&gt; D1[Address: Beijing]
        A2[User ID: 2] --&gt; B2[Name: Li Si] --&gt; C2[Age: 30] --&gt; D2[Address: Shanghai]
        A3[User ID: 3] --&gt; B3[Name: Wang Wu] --&gt; C3[Age: 28] --&gt; D3[Address: Guangzhou]
    end

    subgraph Column_Store[Column Storage]
        direction TB
        A4[User ID] --&gt; A5[1] --&gt; A6[2] --&gt; A7[3]
        B4[Name] --&gt; B5[Zhang San] --&gt; B6[Li Si] --&gt; B7[Wang Wu]
        C4[Age] --&gt; C5[25] --&gt; C6[30] --&gt; C7[28]
        D4[Address] --&gt; D5[Beijing] --&gt; D6[Shanghai] --&gt; D7[Guangzhou]
    end

    class Row_Store row_style;
    class Column_Store column_style;

    classDef row_style, stroke-width:2px;
    classDef column_style, stroke-width:2px;
</div>

<h3 id="join-operation">Join Operation</h3>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> 
    <span class="n">p</span><span class="p">.</span><span class="n">product_name</span><span class="p">,</span>
    <span class="k">SUM</span><span class="p">(</span><span class="n">o</span><span class="p">.</span><span class="n">amount</span><span class="p">)</span> <span class="k">AS</span> <span class="n">total_sales</span>
<span class="k">FROM</span> 
    <span class="n">orders</span> <span class="n">o</span>
<span class="k">JOIN</span> 
    <span class="n">products</span> <span class="n">p</span>
<span class="k">ON</span> 
    <span class="n">o</span><span class="p">.</span><span class="n">product_id</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="n">product_id</span>
<span class="k">GROUP</span> <span class="k">BY</span> 
    <span class="n">p</span><span class="p">.</span><span class="n">product_name</span><span class="p">;</span>
</code></pre></div></div>

<p>This query will be executed in the following steps:</p>

<div class="mermaid">
graph TD
    subgraph Node1[Node 1]
        direction TB
        P1[Read Partition 1 Data] --&gt; A2[Read Data from orders table]
        P2[Read Partition 1 Data] --&gt; A3[Read Data from products table]
        A2 --&gt; B1[Local JOIN]
        A3 --&gt; B1[Local JOIN]
        B1 --&gt; C1[Local Aggregation]
    end

    subgraph Node2[Node 2]
        direction TB
        P3[Read Partition 2 Data] --&gt; A4[Read Data from orders table]
        P4[Read Partition 2 Data] --&gt; A5[Read Data from products table]
        A4 --&gt; B2[Local JOIN]
        A5 --&gt; B2[Local JOIN]
        B2 --&gt; C2[Local Aggregation]
    end

    subgraph NodeN[Node N]
        direction TB
        Pn[Read Partition N Data] --&gt; A6[Read Data from orders table]
        Pn2[Read Partition N Data] --&gt; A7[Read Data from products table]
        A6 --&gt; BN[Local JOIN]
        A7 --&gt; BN[Local JOIN]
        BN --&gt; CN[Local Aggregation]
    end

    C1 --&gt; D1[Global Aggregation]
    C2 --&gt; D1[Global Aggregation]
    CN --&gt; D1[Global Aggregation]

    D1 --&gt; E1[Return Final Result]

    classDef query_step fill:#f9f,stroke:#333,stroke-width:2px;
    class A2,A3,A4,A5,A6,A7,B1,B2,BN,C1,C2,CN,D1,E1 query_step;
</div>

<h3 id="data-structure-and-advantages-or-disadvantages">data structure and advantages or disadvantages</h3>

<table>
  <thead>
    <tr>
      <th><strong>Data Type</strong></th>
      <th><strong>Description</strong></th>
      <th><strong>Advantages</strong></th>
      <th><strong>Disadvantages</strong></th>
      <th><strong>Use Cases</strong></th>
      <th><strong>Query Difficulty</strong></th>
      <th><strong>Insert Difficulty</strong></th>
      <th><strong>Aggregate Analysis Difficulty</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Array</strong></td>
      <td>Stores multiple elements of the same data type, such as integers, strings, etc.</td>
      <td>- Flexible, supports various data types. <br /> - Efficient for querying and manipulating array elements.</td>
      <td>- Does not support heterogeneous data (only stores elements of the same type). <br /> - Performance may degrade with a large number of elements.</td>
      <td>- Storing related values like multiple tags, products, etc. <br /> - Storing bulk data.</td>
      <td>Simple, supports direct indexing and element access.</td>
      <td>Easy, insert multiple values directly.</td>
      <td>Aggregation can be resource-intensive with large data.</td>
    </tr>
    <tr>
      <td><strong>Tuple</strong></td>
      <td>Stores a fixed number of elements with different types.</td>
      <td>- Supports heterogeneous data, number of elements is fixed. <br /> - Suitable for storing composite data fields.</td>
      <td>- Fixed number of elements, not suitable for dynamic changes. <br /> - Does not support dynamically altering data structure.</td>
      <td>- Storing user information, composite fields, tags, etc. <br /> - Suitable for storing structured data.</td>
      <td>Simple, can access elements by position.</td>
      <td>Simple, directly insert a tuple.</td>
      <td>Aggregation can handle individual element aggregation directly.</td>
    </tr>
    <tr>
      <td><strong>LowCardinality</strong></td>
      <td>Used for columns with a low cardinality (few unique values), compressing and mapping values to integers to save memory.</td>
      <td>- Significantly reduces memory and storage usage, especially for repeated data. <br /> - Increases query speed.</td>
      <td>- Only suitable for low-cardinality data. <br /> - May affect query sorting and comparison operations.</td>
      <td>- Suitable for data with a small number of unique values, like cities, gender, categories.</td>
      <td>Good query performance, especially for filtering operations.</td>
      <td>Easy to insert, suitable for bulk insertion of low-cardinality data.</td>
      <td>High efficiency in aggregation.</td>
    </tr>
    <tr>
      <td><strong>UUID</strong></td>
      <td>Universal Unique Identifier (128-bit identifier), typically used for globally unique identifiers.</td>
      <td>- Ensures globally unique identification, avoids ID collisions. <br /> - Common in distributed systems.</td>
      <td>- Large storage space, query performance may be slower, especially in sorting and indexing operations.</td>
      <td>- ID in distributed systems. <br /> - Unique identifier scenarios, such as order IDs, session IDs.</td>
      <td>Slower for queries, especially involving sorting.</td>
      <td>Easy to insert, suitable for globally unique identifiers.</td>
      <td>Aggregation is not difficult, but performance may be slower with large data.</td>
    </tr>
    <tr>
      <td><strong>DateTime</strong></td>
      <td>Stores date and time with second precision.</td>
      <td>- Suitable for timestamp, time-series data. <br /> - Provides rich time handling functions.</td>
      <td>- Relatively larger storage space. <br /> - Only precise to the second, no higher precision.</td>
      <td>- Log analysis, event tracking, time range queries, etc. <br /> - Data partitioning by time.</td>
      <td>High performance for range queries based on time.</td>
      <td>Easy to insert, especially with consistent date format.</td>
      <td>High performance for aggregation by time.</td>
    </tr>
    <tr>
      <td><strong>Date</strong></td>
      <td>Stores only the date (precision to day).</td>
      <td>- Smaller storage space, efficient for date-only data. <br /> - Efficient for date queries and aggregation.</td>
      <td>- Cannot handle time details (like hours, minutes, seconds).</td>
      <td>- Storing date data, such as birth dates, event dates, log dates, etc.</td>
      <td>Extremely efficient for date range queries.</td>
      <td>Simple to insert, just provide date data.</td>
      <td>High performance for day/month aggregation.</td>
    </tr>
    <tr>
      <td><strong>String</strong></td>
      <td>Stores variable-length strings.</td>
      <td>- Highly flexible, supports various text data. <br /> - Suitable for textual fields.</td>
      <td>- Storage and query performance may be slower, especially for large text data. <br /> - Not suitable for structured or complex data.</td>
      <td>- Storing user names, product descriptions, addresses, etc. <br /> - Suitable for unstructured textual data.</td>
      <td>Slower for queries involving large datasets.</td>
      <td>Easy to insert, suitable for general text data.</td>
      <td>Performance may be affected with large text data in aggregation.</td>
    </tr>
    <tr>
      <td><strong>Map</strong></td>
      <td>Stores a collection of key-value pairs, where keys and values can be of different types.</td>
      <td>- Extremely flexible, suitable for storing dynamic and changing data structures. <br /> - Supports fast lookups by key.</td>
      <td>- Higher complexity, querying may be slower, especially when there are many different key-value pairs.</td>
      <td>- Storing dynamic key-value pairs like user attributes, configurations, log information, etc.</td>
      <td>Querying is more complex, involves key-value lookups.</td>
      <td>Relatively easy to insert dynamic key-value pairs.</td>
      <td>Aggregation may affect performance when handling a large number of key-value pairs.</td>
    </tr>
    <tr>
      <td><strong>Nested</strong></td>
      <td>Stores hierarchical data, typically used to represent many-to-many or one-to-many relationships, similar to JSON arrays.</td>
      <td>- Very suitable for storing complex, hierarchical data. <br /> - Can handle complex relationships like users and orders.</td>
      <td>- Querying and manipulating nested data may be complex, especially with large datasets; <code class="language-plaintext highlighter-rouge">ARRAY JOIN</code> may affect performance.</td>
      <td>- Storing users with multiple orders, multi-dimensional data, etc. <br /> - Suitable for many-to-many and hierarchical data.</td>
      <td>Querying requires <code class="language-plaintext highlighter-rouge">ARRAY JOIN</code> to flatten data, relatively complex.</td>
      <td>Slightly complex, data must match the nested format.</td>
      <td>Aggregation can be done by level but requires <code class="language-plaintext highlighter-rouge">ARRAY JOIN</code>, which complicates queries.</td>
    </tr>
  </tbody>
</table>

<h3 id="shard-and-partition-distribution-in-tables">Shard and Partition Distribution in Tables</h3>

<p>In ClickHouse, data is distributed across multiple nodes or shards in a cluster. Each shard is a physical unit of storage that may contain multiple partitions, which help organize the data logically.</p>

<p>For example, consider a table <code class="language-plaintext highlighter-rouge">sales_data</code> that is distributed across three shards. Each shard might contain data for different time periods or geographic regions, allowing queries to target only relevant data for performance optimization.</p>

<h3 id="mergetree-tables-and-data-storage">MergeTree Tables and Data Storage</h3>

<p>The <strong>MergeTree</strong> family of tables is ClickHouse’s primary storage engine. Data in MergeTree tables is organized in parts, where each part corresponds to a subset of data stored in files on disk. Each part contains multiple rows of data and is stored in the columnar format.</p>

<p>And this is the data structure of MergeTree tables:</p>

<div class="mermaid">
graph TD
    A[MergeTree Table] --&gt; B[Parts]
    B --&gt; C[Data Stored in Parts Columnar Format]
    C --&gt; D[Sorted by Primary Key]
    D --&gt; E[Primary Key Index]
    B --&gt; F[Background Merging]
    F --&gt; G[Merges Smaller Parts to Larger Parts]
    A --&gt; H[Partitions]
    H --&gt; I[Data Partitioned by Key （e.g., Date,region]
    A --&gt; J[Marking and Removing Old Data]
    J --&gt; K[TTL or Manual Deletion]
    A --&gt; L[Aggregation and Indexing]
    L --&gt; M[Efficient Querying with Aggregated Data]
    classDef mergeTree fill:#ccf,stroke:#333,stroke-width:2px;
    class A,B,C,D,E,F,G,H,I,J,K,L,M mergeTree;
</div>

<p><strong>MergeTree tables</strong> in ClickHouse are designed to handle large volumes of data while optimizing storage and query performance. Below are key features that contribute to these optimizations:</p>

<ol>
  <li>
    <p><strong>Primary Key</strong>:<br />
The data within each part of a <strong>MergeTree</strong> table is organized and sorted by a <strong>primary key</strong>. The primary key index helps with the efficient retrieval of rows and improves query performance, especially for range queries. It also enables the storage of data in an ordered fashion, which is critical for performing quick searches and aggregations.</p>
  </li>
  <li><strong>Parts Merging</strong>:<br />
Over time, as new data is inserted, smaller parts are created. These parts are periodically merged in the background into <strong>larger parts</strong>. This merging process helps to:
    <ul>
      <li><strong>Reduce disk fragmentation</strong> by consolidating smaller data parts.</li>
      <li><strong>Improve read and write performance</strong> by optimizing how data is accessed.</li>
      <li><strong>Maintain optimal storage utilization</strong>, ensuring that large datasets are organized efficiently, leading to faster query execution.</li>
    </ul>
  </li>
  <li>
    <p><strong>Columnar Storage</strong>:<br />
Data is stored in a <strong>columnar format</strong> within each part, making it ideal for <strong>analytical queries</strong>. This format allows ClickHouse to read only the relevant columns for a query, which speeds up the query execution and reduces resource usage.</p>
  </li>
  <li><strong>Partitioning</strong>:<br />
The table is divided into <strong>partitions</strong>, which are logical subdivisions of the data, often based on a key (e.g., date or region). Partitioning helps in:
    <ul>
      <li>Organizing data efficiently for faster querying.</li>
      <li>Enabling better management and querying of data over time, such as time-series data.</li>
      <li>Reducing the number of rows to scan, as only the relevant partitions are queried.</li>
    </ul>
  </li>
  <li><strong>TTL (Time-to-Live) and Manual Data Deletion</strong>:<br />
<strong>TTL (Time-to-Live)</strong> is a feature that allows data to be automatically deleted after a certain period. This is particularly useful for log or time-series data that becomes irrelevant over time.
    <ul>
      <li><strong>Manual deletion</strong> can also be performed on old data or specific partitions that are no longer needed, freeing up disk space and improving query performance.</li>
    </ul>
  </li>
  <li><strong>Aggregation and Indexing</strong>:<br />
<strong>Pre-aggregated data</strong> and <strong>secondary indexes</strong> can be created to speed up queries that involve heavy aggregations or filtering. This helps in:
    <ul>
      <li><strong>Efficient querying with aggregated data</strong>, as results can be retrieved directly from pre-aggregated values rather than recalculating them every time a query is run.</li>
      <li><strong>Improving query performance</strong> by reducing the need to perform complex calculations during query execution.</li>
    </ul>
  </li>
</ol>

<h3 id="partitioning-strategy">Partitioning Strategy</h3>

<p>ClickHouse uses partitioning to divide large datasets into smaller, more manageable chunks. Typically, partitions are created by time, which makes it easier to prune data when querying recent data or performing time-based aggregations.</p>

<p>For example, in a large <code class="language-plaintext highlighter-rouge">clickstream_data</code> table, partitions could be created by day or week, improving query performance when analyzing recent activity.</p>

<h2 id="clickhouses-internal-architecture">ClickHouse’s Internal Architecture</h2>

<p>ClickHouse operates on a distributed architecture, where data is spread across multiple nodes in a cluster. This distributed design ensures high availability, fault tolerance, and scalability.</p>

<h3 id="key-components">Key Components</h3>

<ul>
  <li><strong>Shard</strong>: A unit of storage, representing a physical machine or node in the cluster.</li>
  <li><strong>Replica</strong>: A copy of a shard’s data for redundancy. ClickHouse ensures that replicas are in sync to provide fault tolerance.</li>
  <li><strong>Distributed Table</strong>: A virtual table that abstracts access to data stored across multiple shards and replicas.</li>
  <li><strong>ZooKeeper</strong>: Used for managing metadata, leader election, and coordinating replica synchronization.</li>
</ul>

<div class="mermaid">
graph TD
  A[Producer] --&gt; B[Distributed Table]
  B --&gt; C[Shard]
  C --&gt; D[Replica]
  D --&gt; E[Consumer]
  E --&gt; F[ZooKeeper]
</div>

<h2 id="ensuring-high-reliability">Ensuring High Reliability</h2>

<p>ClickHouse’s high availability and fault tolerance are guaranteed through its replication and synchronization strategies. These mechanisms ensure that even in the case of node failures, data is not lost and the system remains operational.</p>

<h3 id="data-replication">Data Replication</h3>

<p>Each shard in ClickHouse can have one or more replicas. Replicas store identical data and are synchronized with the leader shard. If a node or replica fails, the system can continue operating by redirecting queries to another replica.</p>

<div class="mermaid">
graph TD
  A[Producer] --&gt; B[Leader Shard]
  B --&gt; C[Replica 1]
  B --&gt; D[Replica 2]
  C --&gt; E[Write Sync]
  D --&gt; E[Write Sync]
</div>

<h3 id="replica-synchronization">Replica Synchronization</h3>

<p>Replicas synchronize their data using a process where changes made to the leader shard are propagated to the replicas. This ensures that all replicas are consistent and up-to-date with the leader shard.</p>

<h3 id="partition-replication-and-fault-tolerance">Partition Replication and Fault Tolerance</h3>

<p>ClickHouse provides fault tolerance through the replication of entire partitions. If a partition becomes unavailable on one replica due to a failure, another replica with the same data can serve the request without any downtime.</p>

<h3 id="leader-election-and-failover">Leader Election and Failover</h3>

<p>ZooKeeper is used for leader election in ClickHouse. When a replica fails, ZooKeeper helps determine which replica should take over as the leader. This ensures that the system can continue serving requests without interruption.</p>

<div class="mermaid">
graph TD
  A[ZooKeeper] --&gt; B[Leader Election]
  B --&gt; C[Replica 1]
  B --&gt; D[Replica 2]
  C --&gt; E[Leader Role]
  D --&gt; F[Follower Role]
</div>

<h2 id="clickhouses-distributed-query-execution">ClickHouse’s Distributed Query Execution</h2>

<p>ClickHouse can execute queries in parallel across multiple shards and replicas, ensuring that even complex queries over large datasets are processed efficiently.</p>

<h3 id="distributed-query-execution-plan">Distributed Query Execution Plan</h3>

<p>When a query is executed, the <strong>Distributed Table</strong> abstraction allows ClickHouse to create a query plan that targets the relevant shards and replicas. The query is broken down and sent to each shard in the cluster, which processes the query locally. Results from all shards are then aggregated and returned to the user.</p>

<h3 id="materialized-views-for-optimization">Materialized Views for Optimization</h3>

<p>ClickHouse supports <strong>materialized views</strong>, which are precomputed results of a query that are stored in the database. Materialized views are used to speed up query execution, especially for complex aggregations or frequent queries. Once a materialized view is created, ClickHouse updates it automatically when the underlying data changes.</p>

<h2 id="conclusion">Conclusion</h2>

<p>ClickHouse’s distributed architecture and OLAP capabilities make it an ideal choice for real-time data analytics. With features like partitioning, replication, and parallel query execution, it is well-suited for high-performance workloads. ClickHouse ensures high availability and fault tolerance, while also offering powerful mechanisms to scale out horizontally and optimize queries.</p>

<p>The system’s robust design ensures that users can efficiently manage and analyze vast amounts of data in real time, making ClickHouse a critical tool for modern data-driven applications.</p>

<script type="module">
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
  mermaid.initialize({ startOnLoad: true });
</script>]]></content><author><name>Madden Zhang</name></author><category term="Blog" /><category term="distribute" /><category term="clickhouse" /><summary type="html"><![CDATA[Introduction ClickHouse, an open-source columnar database management system (DBMS), is designed for online analytical processing (OLAP). It provides real-time data analysis with extremely fast query performance, even on massive datasets. Originally developed by Yandex, ClickHouse has become a popular solution for analytics-heavy use cases in large-scale environments, supporting high throughput and low-latency queries. ClickHouse’s architecture is engineered for distributed processing, offering both horizontal scalability and fault tolerance. This article delves into ClickHouse’s architecture and its core features, including data distribution, replication, and fault tolerance mechanisms. Key Terminologies Term Explanation Shard A physical subset of data that is stored on a specific set of nodes. Replica A copy of a shard’s data for fault tolerance and high availability. Table A collection of data organized into columns. MergeTree A special type of table that supports fast querying and efficient storage. Partition A logical division of data within a table, often mapped to a specific time period. Distributed Table A table that abstracts data stored on different nodes in a cluster. Replica Synchronization The process by which replicas stay in sync with their leader shard. Materialized View A precomputed view that stores the result of a query to optimize query performance. ClickHouse’s Storage Mechanism Columnar Storage ClickHouse’s storage engine is optimized for OLAP workloads. It stores data in columnar format, which is ideal for analytical queries that often only access a few columns at a time. This section explores the data distribution, partitioning, and table storage mechanisms in ClickHouse. What is the the difference between row format and columnar format? graph TD subgraph Row_Store[Row Storage] direction LR A1[User ID: 1] --&gt; B1[Name: Zhang San] --&gt; C1[Age: 25] --&gt; D1[Address: Beijing] A2[User ID: 2] --&gt; B2[Name: Li Si] --&gt; C2[Age: 30] --&gt; D2[Address: Shanghai] A3[User ID: 3] --&gt; B3[Name: Wang Wu] --&gt; C3[Age: 28] --&gt; D3[Address: Guangzhou] end subgraph Column_Store[Column Storage] direction TB A4[User ID] --&gt; A5[1] --&gt; A6[2] --&gt; A7[3] B4[Name] --&gt; B5[Zhang San] --&gt; B6[Li Si] --&gt; B7[Wang Wu] C4[Age] --&gt; C5[25] --&gt; C6[30] --&gt; C7[28] D4[Address] --&gt; D5[Beijing] --&gt; D6[Shanghai] --&gt; D7[Guangzhou] end class Row_Store row_style; class Column_Store column_style; classDef row_style, stroke-width:2px; classDef column_style, stroke-width:2px; Join Operation SELECT p.product_name, SUM(o.amount) AS total_sales FROM orders o JOIN products p ON o.product_id = p.product_id GROUP BY p.product_name; This query will be executed in the following steps: graph TD subgraph Node1[Node 1] direction TB P1[Read Partition 1 Data] --&gt; A2[Read Data from orders table] P2[Read Partition 1 Data] --&gt; A3[Read Data from products table] A2 --&gt; B1[Local JOIN] A3 --&gt; B1[Local JOIN] B1 --&gt; C1[Local Aggregation] end subgraph Node2[Node 2] direction TB P3[Read Partition 2 Data] --&gt; A4[Read Data from orders table] P4[Read Partition 2 Data] --&gt; A5[Read Data from products table] A4 --&gt; B2[Local JOIN] A5 --&gt; B2[Local JOIN] B2 --&gt; C2[Local Aggregation] end subgraph NodeN[Node N] direction TB Pn[Read Partition N Data] --&gt; A6[Read Data from orders table] Pn2[Read Partition N Data] --&gt; A7[Read Data from products table] A6 --&gt; BN[Local JOIN] A7 --&gt; BN[Local JOIN] BN --&gt; CN[Local Aggregation] end C1 --&gt; D1[Global Aggregation] C2 --&gt; D1[Global Aggregation] CN --&gt; D1[Global Aggregation] D1 --&gt; E1[Return Final Result] classDef query_step fill:#f9f,stroke:#333,stroke-width:2px; class A2,A3,A4,A5,A6,A7,B1,B2,BN,C1,C2,CN,D1,E1 query_step; data structure and advantages or disadvantages Data Type Description Advantages Disadvantages Use Cases Query Difficulty Insert Difficulty Aggregate Analysis Difficulty Array Stores multiple elements of the same data type, such as integers, strings, etc. - Flexible, supports various data types. - Efficient for querying and manipulating array elements. - Does not support heterogeneous data (only stores elements of the same type). - Performance may degrade with a large number of elements. - Storing related values like multiple tags, products, etc. - Storing bulk data. Simple, supports direct indexing and element access. Easy, insert multiple values directly. Aggregation can be resource-intensive with large data. Tuple Stores a fixed number of elements with different types. - Supports heterogeneous data, number of elements is fixed. - Suitable for storing composite data fields. - Fixed number of elements, not suitable for dynamic changes. - Does not support dynamically altering data structure. - Storing user information, composite fields, tags, etc. - Suitable for storing structured data. Simple, can access elements by position. Simple, directly insert a tuple. Aggregation can handle individual element aggregation directly. LowCardinality Used for columns with a low cardinality (few unique values), compressing and mapping values to integers to save memory. - Significantly reduces memory and storage usage, especially for repeated data. - Increases query speed. - Only suitable for low-cardinality data. - May affect query sorting and comparison operations. - Suitable for data with a small number of unique values, like cities, gender, categories. Good query performance, especially for filtering operations. Easy to insert, suitable for bulk insertion of low-cardinality data. High efficiency in aggregation. UUID Universal Unique Identifier (128-bit identifier), typically used for globally unique identifiers. - Ensures globally unique identification, avoids ID collisions. - Common in distributed systems. - Large storage space, query performance may be slower, especially in sorting and indexing operations. - ID in distributed systems. - Unique identifier scenarios, such as order IDs, session IDs. Slower for queries, especially involving sorting. Easy to insert, suitable for globally unique identifiers. Aggregation is not difficult, but performance may be slower with large data. DateTime Stores date and time with second precision. - Suitable for timestamp, time-series data. - Provides rich time handling functions. - Relatively larger storage space. - Only precise to the second, no higher precision. - Log analysis, event tracking, time range queries, etc. - Data partitioning by time. High performance for range queries based on time. Easy to insert, especially with consistent date format. High performance for aggregation by time. Date Stores only the date (precision to day). - Smaller storage space, efficient for date-only data. - Efficient for date queries and aggregation. - Cannot handle time details (like hours, minutes, seconds). - Storing date data, such as birth dates, event dates, log dates, etc. Extremely efficient for date range queries. Simple to insert, just provide date data. High performance for day/month aggregation. String Stores variable-length strings. - Highly flexible, supports various text data. - Suitable for textual fields. - Storage and query performance may be slower, especially for large text data. - Not suitable for structured or complex data. - Storing user names, product descriptions, addresses, etc. - Suitable for unstructured textual data. Slower for queries involving large datasets. Easy to insert, suitable for general text data. Performance may be affected with large text data in aggregation. Map Stores a collection of key-value pairs, where keys and values can be of different types. - Extremely flexible, suitable for storing dynamic and changing data structures. - Supports fast lookups by key. - Higher complexity, querying may be slower, especially when there are many different key-value pairs. - Storing dynamic key-value pairs like user attributes, configurations, log information, etc. Querying is more complex, involves key-value lookups. Relatively easy to insert dynamic key-value pairs. Aggregation may affect performance when handling a large number of key-value pairs. Nested Stores hierarchical data, typically used to represent many-to-many or one-to-many relationships, similar to JSON arrays. - Very suitable for storing complex, hierarchical data. - Can handle complex relationships like users and orders. - Querying and manipulating nested data may be complex, especially with large datasets; ARRAY JOIN may affect performance. - Storing users with multiple orders, multi-dimensional data, etc. - Suitable for many-to-many and hierarchical data. Querying requires ARRAY JOIN to flatten data, relatively complex. Slightly complex, data must match the nested format. Aggregation can be done by level but requires ARRAY JOIN, which complicates queries. Shard and Partition Distribution in Tables In ClickHouse, data is distributed across multiple nodes or shards in a cluster. Each shard is a physical unit of storage that may contain multiple partitions, which help organize the data logically. For example, consider a table sales_data that is distributed across three shards. Each shard might contain data for different time periods or geographic regions, allowing queries to target only relevant data for performance optimization. MergeTree Tables and Data Storage The MergeTree family of tables is ClickHouse’s primary storage engine. Data in MergeTree tables is organized in parts, where each part corresponds to a subset of data stored in files on disk. Each part contains multiple rows of data and is stored in the columnar format. And this is the data structure of MergeTree tables: graph TD A[MergeTree Table] --&gt; B[Parts] B --&gt; C[Data Stored in Parts Columnar Format] C --&gt; D[Sorted by Primary Key] D --&gt; E[Primary Key Index] B --&gt; F[Background Merging] F --&gt; G[Merges Smaller Parts to Larger Parts] A --&gt; H[Partitions] H --&gt; I[Data Partitioned by Key （e.g., Date,region] A --&gt; J[Marking and Removing Old Data] J --&gt; K[TTL or Manual Deletion] A --&gt; L[Aggregation and Indexing] L --&gt; M[Efficient Querying with Aggregated Data] classDef mergeTree fill:#ccf,stroke:#333,stroke-width:2px; class A,B,C,D,E,F,G,H,I,J,K,L,M mergeTree; MergeTree tables in ClickHouse are designed to handle large volumes of data while optimizing storage and query performance. Below are key features that contribute to these optimizations: Primary Key: The data within each part of a MergeTree table is organized and sorted by a primary key. The primary key index helps with the efficient retrieval of rows and improves query performance, especially for range queries. It also enables the storage of data in an ordered fashion, which is critical for performing quick searches and aggregations. Parts Merging: Over time, as new data is inserted, smaller parts are created. These parts are periodically merged in the background into larger parts. This merging process helps to: Reduce disk fragmentation by consolidating smaller data parts. Improve read and write performance by optimizing how data is accessed. Maintain optimal storage utilization, ensuring that large datasets are organized efficiently, leading to faster query execution. Columnar Storage: Data is stored in a columnar format within each part, making it ideal for analytical queries. This format allows ClickHouse to read only the relevant columns for a query, which speeds up the query execution and reduces resource usage. Partitioning: The table is divided into partitions, which are logical subdivisions of the data, often based on a key (e.g., date or region). Partitioning helps in: Organizing data efficiently for faster querying. Enabling better management and querying of data over time, such as time-series data. Reducing the number of rows to scan, as only the relevant partitions are queried. TTL (Time-to-Live) and Manual Data Deletion: TTL (Time-to-Live) is a feature that allows data to be automatically deleted after a certain period. This is particularly useful for log or time-series data that becomes irrelevant over time. Manual deletion can also be performed on old data or specific partitions that are no longer needed, freeing up disk space and improving query performance. Aggregation and Indexing: Pre-aggregated data and secondary indexes can be created to speed up queries that involve heavy aggregations or filtering. This helps in: Efficient querying with aggregated data, as results can be retrieved directly from pre-aggregated values rather than recalculating them every time a query is run. Improving query performance by reducing the need to perform complex calculations during query execution. Partitioning Strategy ClickHouse uses partitioning to divide large datasets into smaller, more manageable chunks. Typically, partitions are created by time, which makes it easier to prune data when querying recent data or performing time-based aggregations. For example, in a large clickstream_data table, partitions could be created by day or week, improving query performance when analyzing recent activity. ClickHouse’s Internal Architecture ClickHouse operates on a distributed architecture, where data is spread across multiple nodes in a cluster. This distributed design ensures high availability, fault tolerance, and scalability. Key Components Shard: A unit of storage, representing a physical machine or node in the cluster. Replica: A copy of a shard’s data for redundancy. ClickHouse ensures that replicas are in sync to provide fault tolerance. Distributed Table: A virtual table that abstracts access to data stored across multiple shards and replicas. ZooKeeper: Used for managing metadata, leader election, and coordinating replica synchronization. graph TD A[Producer] --&gt; B[Distributed Table] B --&gt; C[Shard] C --&gt; D[Replica] D --&gt; E[Consumer] E --&gt; F[ZooKeeper] Ensuring High Reliability ClickHouse’s high availability and fault tolerance are guaranteed through its replication and synchronization strategies. These mechanisms ensure that even in the case of node failures, data is not lost and the system remains operational. Data Replication Each shard in ClickHouse can have one or more replicas. Replicas store identical data and are synchronized with the leader shard. If a node or replica fails, the system can continue operating by redirecting queries to another replica. graph TD A[Producer] --&gt; B[Leader Shard] B --&gt; C[Replica 1] B --&gt; D[Replica 2] C --&gt; E[Write Sync] D --&gt; E[Write Sync] Replica Synchronization Replicas synchronize their data using a process where changes made to the leader shard are propagated to the replicas. This ensures that all replicas are consistent and up-to-date with the leader shard. Partition Replication and Fault Tolerance ClickHouse provides fault tolerance through the replication of entire partitions. If a partition becomes unavailable on one replica due to a failure, another replica with the same data can serve the request without any downtime. Leader Election and Failover ZooKeeper is used for leader election in ClickHouse. When a replica fails, ZooKeeper helps determine which replica should take over as the leader. This ensures that the system can continue serving requests without interruption. graph TD A[ZooKeeper] --&gt; B[Leader Election] B --&gt; C[Replica 1] B --&gt; D[Replica 2] C --&gt; E[Leader Role] D --&gt; F[Follower Role] ClickHouse’s Distributed Query Execution ClickHouse can execute queries in parallel across multiple shards and replicas, ensuring that even complex queries over large datasets are processed efficiently. Distributed Query Execution Plan When a query is executed, the Distributed Table abstraction allows ClickHouse to create a query plan that targets the relevant shards and replicas. The query is broken down and sent to each shard in the cluster, which processes the query locally. Results from all shards are then aggregated and returned to the user. Materialized Views for Optimization ClickHouse supports materialized views, which are precomputed results of a query that are stored in the database. Materialized views are used to speed up query execution, especially for complex aggregations or frequent queries. Once a materialized view is created, ClickHouse updates it automatically when the underlying data changes. Conclusion ClickHouse’s distributed architecture and OLAP capabilities make it an ideal choice for real-time data analytics. With features like partitioning, replication, and parallel query execution, it is well-suited for high-performance workloads. ClickHouse ensures high availability and fault tolerance, while also offering powerful mechanisms to scale out horizontally and optimize queries. The system’s robust design ensures that users can efficiently manage and analyze vast amounts of data in real time, making ClickHouse a critical tool for modern data-driven applications.]]></summary></entry><entry><title type="html">My Efficient Tools for Development as a Developer</title><link href="http://localhost:4000/blog/efficient-development-tools/" rel="alternate" type="text/html" title="My Efficient Tools for Development as a Developer" /><published>2025-02-14T00:00:00+08:00</published><updated>2025-02-14T00:00:00+08:00</updated><id>http://localhost:4000/blog/efficient-development-tools</id><content type="html" xml:base="http://localhost:4000/blog/efficient-development-tools/"><![CDATA[<h2 id="abstract">Abstract</h2>

<p>In this article, I will introduce a range of tools that enhance the productivity of software developers. These tools span across multiple domains such as code editors, version control, debugging, project management, and more. I will not only highlight these tools but also explain how to configure them and introduce key plugins that optimize the development process.</p>

<hr />

<h2 id="1-text-editors--integrated-development-environments-ides">1. <strong>Text Editors &amp; Integrated Development Environments (IDEs)</strong></h2>

<h3 id="11-vs-code">1.1 <strong>VS Code</strong></h3>

<p><strong>Overview:</strong><br />
Visual Studio Code (VS Code) is a popular, lightweight, and powerful code editor developed by Microsoft. It supports numerous programming languages and comes with built-in Git integration.</p>

<p><strong>Configuration:</strong></p>
<ul>
  <li><strong>Install VS Code</strong>: Download the installer from <a href="https://code.visualstudio.com/">VS Code’s official site</a>.</li>
  <li><strong>Extensions</strong>:
    <ul>
      <li><strong>Prettier</strong>: Auto-format code to maintain consistent style.</li>
      <li><strong>ESLint</strong>: Linting tool for JavaScript to catch syntax and style errors.</li>
      <li><strong>GitLens</strong>: Enhances Git integration, providing inline blame and history.</li>
    </ul>
  </li>
</ul>

<p><strong>Plugins:</strong></p>
<ol>
  <li><strong>Live Server</strong>: Launch a local development server with a live reload feature for static or dynamic pages.</li>
  <li><strong>Debugger for Chrome</strong>: Attach the VS Code debugger to Chrome for front-end debugging.</li>
</ol>

<hr />

<h3 id="12-jetbrains-intellij-idea">1.2 <strong>JetBrains IntelliJ IDEA</strong></h3>

<p><strong>Overview:</strong><br />
IntelliJ IDEA is a robust and full-featured IDE, especially popular for Java development, but also supports a wide range of other languages such as Kotlin, Python, and JavaScript.</p>

<p><strong>Configuration:</strong></p>
<ul>
  <li><strong>Install IntelliJ IDEA</strong>: Download from the <a href="https://www.jetbrains.com/idea/">JetBrains website</a>.</li>
  <li><strong>Plugins</strong>:
    <ul>
      <li><strong>Lombok Plugin</strong>: Supports the Lombok annotation library for Java.</li>
      <li><strong>Spring Boot</strong>: For Spring Boot framework development.</li>
      <li><strong>Database Navigator</strong>: Allows database management and direct queries from the IDE.</li>
    </ul>
  </li>
</ul>

<p><strong>Plugins:</strong></p>
<ol>
  <li><strong>CheckStyle-IDEA</strong>: A plugin for integrating CheckStyle, a static code analysis tool for Java.</li>
  <li><strong>Docker</strong>: Supports running and managing Docker containers within the IDE.</li>
</ol>

<hr />

<h3 id="13-vim--neovim">1.3 <strong>Vim / Neovim</strong></h3>

<p><strong>Overview:</strong><br />
Vim is a highly customizable terminal-based text editor known for its efficiency and steep learning curve. Neovim is an enhanced fork of Vim with additional features and plugin support.</p>

<p><strong>Configuration:</strong></p>
<ul>
  <li><strong>Install Vim/Neovim</strong>: Vim comes pre-installed on most Unix systems. Neovim can be installed via package managers like <code class="language-plaintext highlighter-rouge">brew</code> on macOS.</li>
  <li><strong>Key Plugin</strong>:
    <ul>
      <li><strong>vim-plug</strong>: A plugin manager for managing and installing plugins in Vim/Neovim. To install vim-plug, add the following to your <code class="language-plaintext highlighter-rouge">~/.vimrc</code> or <code class="language-plaintext highlighter-rouge">~/.config/nvim/init.vim</code>:
        <div class="language-vim highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">call</span> plug#begin<span class="p">(</span><span class="s1">'~/.vim/plugged'</span><span class="p">)</span>
Plug <span class="s1">'tpope/vim-sensible'</span>  " Sensible default configurations
Plug <span class="s1">'neoclide/coc.nvim'</span>   " Autocompletion <span class="nb">and</span> IntelliSense
<span class="k">call</span> plug#end<span class="p">()</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Plugins:</strong></p>
<ol>
  <li><strong>coc.nvim</strong>: A comprehensive autocompletion plugin providing IntelliSense-like features.</li>
  <li><strong>NERDTree</strong>: A file system explorer for Vim to easily navigate the project structure.</li>
</ol>

<hr />

<h2 id="2-version-control-systems">2. <strong>Version Control Systems</strong></h2>

<h3 id="21-git">2.1 <strong>Git</strong></h3>

<p><strong>Overview:</strong><br />
Git is a distributed version control system that enables teams to work collaboratively, maintain code history, and track changes efficiently.</p>

<p><strong>Configuration:</strong></p>
<ul>
  <li><strong>Install Git</strong>: <a href="https://git-scm.com/">Download Git here</a>.</li>
  <li><strong>Basic Setup</strong>:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git config <span class="nt">--global</span> user.name <span class="s2">"Your Name"</span>
git config <span class="nt">--global</span> user.email <span class="s2">"your_email@example.com"</span>
</code></pre></div>    </div>
  </li>
</ul>

<p><strong>Plugins:</strong></p>
<ol>
  <li><strong>GitLens</strong>: VS Code extension that supercharges Git capabilities with visualizations and history.</li>
  <li><strong>SourceTree</strong>: A GUI for managing Git repositories, providing an intuitive interface for committing, branching, and merging.</li>
</ol>

<hr />

<h2 id="3-project-management-tools">3. <strong>Project Management Tools</strong></h2>

<h3 id="31-trello">3.1 <strong>Trello</strong></h3>

<p><strong>Overview:</strong><br />
Trello is a visual project management tool that uses boards, lists, and cards to organize tasks. It is simple to use but highly effective for personal and team projects.</p>

<p><strong>Configuration:</strong></p>
<ul>
  <li><strong>Set up Trello</strong>: <a href="https://trello.com/">Sign up here</a>.</li>
  <li><strong>Power-Ups</strong>: Enhance Trello boards with Power-Ups (integrations), such as:
    <ul>
      <li><strong>GitHub</strong>: Link GitHub repositories to Trello cards.</li>
      <li><strong>Slack</strong>: Get Trello notifications in Slack channels.</li>
    </ul>
  </li>
</ul>

<p><strong>Plugins:</strong></p>
<ul>
  <li><strong>Butler</strong>: Automate actions within Trello using simple commands (e.g., moving cards based on due dates).</li>
</ul>

<hr />

<h3 id="32-jira">3.2 <strong>Jira</strong></h3>

<p><strong>Overview:</strong><br />
Jira is a comprehensive project management tool designed for software teams, often used in Agile environments to track sprints, issues, and progress.</p>

<p><strong>Configuration:</strong></p>
<ul>
  <li><strong>Set up Jira</strong>: <a href="https://www.atlassian.com/software/jira">Jira Software</a> provides cloud and self-hosted options.</li>
  <li><strong>Integrations</strong>: Integrate Jira with GitHub, Bitbucket, or GitLab to track commits and branches.</li>
</ul>

<p><strong>Plugins:</strong></p>
<ol>
  <li><strong>Tempo Timesheets</strong>: Time tracking within Jira.</li>
  <li><strong>Slack for Jira</strong>: Sync Jira with Slack to get notifications for tasks and issues.</li>
</ol>

<hr />

<h2 id="4-cicd-tools">4. <strong>CI/CD Tools</strong></h2>

<h3 id="41-jenkins">4.1 <strong>Jenkins</strong></h3>

<p><strong>Overview:</strong><br />
Jenkins is an open-source automation server for building, deploying, and automating software projects.</p>

<p><strong>Configuration:</strong></p>
<ul>
  <li><strong>Install Jenkins</strong>: Follow installation instructions from the <a href="https://www.jenkins.io/doc/book/installing/">Jenkins website</a>.</li>
  <li><strong>Pipeline Configuration</strong>: Create a <code class="language-plaintext highlighter-rouge">Jenkinsfile</code> to define a continuous integration pipeline.</li>
</ul>

<p><strong>Plugins:</strong></p>
<ol>
  <li><strong>Git Plugin</strong>: Allows Jenkins to integrate with Git repositories.</li>
  <li><strong>Docker Pipeline Plugin</strong>: Integrates Docker with Jenkins pipelines, allowing Docker containers to be used as build agents.</li>
</ol>

<hr />

<h3 id="42-github-actions">4.2 <strong>GitHub Actions</strong></h3>

<p><strong>Overview:</strong><br />
GitHub Actions is an automation tool that allows you to create custom workflows directly within GitHub repositories, enabling continuous integration and deployment.</p>

<p><strong>Configuration:</strong></p>
<ul>
  <li><strong>Set up a GitHub Action</strong>: Create a <code class="language-plaintext highlighter-rouge">.github/workflows/main.yml</code> file with the workflow definition.<br />
Example:
    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">name</span><span class="pi">:</span> <span class="s">CI</span>

<span class="na">on</span><span class="pi">:</span> <span class="pi">[</span><span class="nv">push</span><span class="pi">]</span>

<span class="na">jobs</span><span class="pi">:</span>
  <span class="na">build</span><span class="pi">:</span>
    <span class="na">runs-on</span><span class="pi">:</span> <span class="s">ubuntu-latest</span>

    <span class="na">steps</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">uses</span><span class="pi">:</span> <span class="s">actions/checkout@v2</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Set up Node.js</span>
        <span class="na">uses</span><span class="pi">:</span> <span class="s">actions/setup-node@v2</span>
        <span class="na">with</span><span class="pi">:</span>
          <span class="na">node-version</span><span class="pi">:</span> <span class="s1">'</span><span class="s">14'</span>
      <span class="pi">-</span> <span class="na">run</span><span class="pi">:</span> <span class="s">npm install</span>
</code></pre></div>    </div>
  </li>
</ul>

<p><strong>Plugins:</strong></p>
<ol>
  <li><strong>actions/setup-node</strong>: A popular action for setting up Node.js in CI pipelines.</li>
  <li><strong>actions/cache</strong>: Cache dependencies to speed up builds.</li>
</ol>

<hr />

<h2 id="5-containerization--virtualization">5. <strong>Containerization &amp; Virtualization</strong></h2>

<h3 id="51-docker">5.1 <strong>Docker</strong></h3>

<p><strong>Overview:</strong><br />
Docker allows developers to package applications and their dependencies into containers, ensuring consistent behavior across different environments.</p>

<p><strong>Configuration:</strong></p>
<ul>
  <li><strong>Install Docker</strong>: <a href="https://www.docker.com/products/docker-desktop">Download Docker</a> for your operating system.</li>
  <li><strong>Basic Dockerfile</strong>:
    <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> node:14</span>
<span class="k">WORKDIR</span><span class="s"> /app</span>
<span class="k">COPY</span><span class="s"> . .</span>
<span class="k">RUN </span>npm <span class="nb">install</span>
<span class="k">CMD</span><span class="s"> ["npm", "start"]</span>
</code></pre></div>    </div>
  </li>
</ul>

<p><strong>Plugins:</strong></p>
<ol>
  <li><strong>Docker Compose</strong>: Tool to define and run multi-container Docker applications.</li>
  <li><strong>Docker Extension for VS Code</strong>: Manage containers, images, and networks directly from the editor.</li>
</ol>

<hr />

<h2 id="6-time-management--focus">6. <strong>Time Management &amp; Focus</strong></h2>

<h3 id="61-pomodone">6.1 <strong>Pomodone</strong></h3>

<p><strong>Overview:</strong><br />
Pomodone is a Pomodoro technique timer that helps developers break work into 25-minute intervals followed by short breaks.</p>

<p><strong>Configuration:</strong></p>
<ul>
  <li><strong>Install Pomodone</strong>: <a href="https://pomodoneapp.com/">Sign up here</a>.</li>
  <li><strong>Integrate with Tools</strong>: Sync tasks from Trello or Asana to Pomodone to manage your workflow effectively.</li>
</ul>

<p><strong>Plugins:</strong></p>
<ol>
  <li><strong>Trello Integration</strong>: Manage tasks from Trello boards and apply the Pomodoro technique.</li>
</ol>

<hr />

<h2 id="7-reading--learning">7. <strong>Reading &amp; Learning</strong></h2>

<h3 id="71-bypass-paywalls-clean">7.1 <strong>Bypass-paywalls-clean</strong></h3>

<p><strong>Overview:</strong><br />
Bypass-paywalls-clean is a browser extension that allows you to access paywalled content for free.</p>

<p><strong>Configuration:</strong></p>
<ul>
  <li>
    <h2 id="install-bypass-paywalls-clean-download"><strong>Install Bypass-paywalls-clean</strong>: <a href="https://gitflic.ru/project/magnolia1234/bpc_uploads">Download</a>.</h2>
  </li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>In this article, I have covered a wide range of productivity tools, from code editors and version control systems to project management and CI/CD tools. Each of these tools has its configuration methods and plugin extensions that can enhance the developer experience. By leveraging these tools and their respective plugins, developers can automate repetitive tasks, streamline their workflows, and ultimately improve productivity.</p>

<hr />

<p>This article serves as an introduction to some of the most effective productivity tools for developers. For detailed configurations and specific use cases, each tool’s official documentation and plugin resources should be consulted.</p>]]></content><author><name>Madden Zhang</name></author><category term="Blog" /><category term="Tools" /><category term="Software" /><category term="Efficiency" /><summary type="html"><![CDATA[Abstract In this article, I will introduce a range of tools that enhance the productivity of software developers. These tools span across multiple domains such as code editors, version control, debugging, project management, and more. I will not only highlight these tools but also explain how to configure them and introduce key plugins that optimize the development process. 1. Text Editors &amp; Integrated Development Environments (IDEs) 1.1 VS Code Overview: Visual Studio Code (VS Code) is a popular, lightweight, and powerful code editor developed by Microsoft. It supports numerous programming languages and comes with built-in Git integration. Configuration: Install VS Code: Download the installer from VS Code’s official site. Extensions: Prettier: Auto-format code to maintain consistent style. ESLint: Linting tool for JavaScript to catch syntax and style errors. GitLens: Enhances Git integration, providing inline blame and history. Plugins: Live Server: Launch a local development server with a live reload feature for static or dynamic pages. Debugger for Chrome: Attach the VS Code debugger to Chrome for front-end debugging. 1.2 JetBrains IntelliJ IDEA Overview: IntelliJ IDEA is a robust and full-featured IDE, especially popular for Java development, but also supports a wide range of other languages such as Kotlin, Python, and JavaScript. Configuration: Install IntelliJ IDEA: Download from the JetBrains website. Plugins: Lombok Plugin: Supports the Lombok annotation library for Java. Spring Boot: For Spring Boot framework development. Database Navigator: Allows database management and direct queries from the IDE. Plugins: CheckStyle-IDEA: A plugin for integrating CheckStyle, a static code analysis tool for Java. Docker: Supports running and managing Docker containers within the IDE. 1.3 Vim / Neovim Overview: Vim is a highly customizable terminal-based text editor known for its efficiency and steep learning curve. Neovim is an enhanced fork of Vim with additional features and plugin support. Configuration: Install Vim/Neovim: Vim comes pre-installed on most Unix systems. Neovim can be installed via package managers like brew on macOS. Key Plugin: vim-plug: A plugin manager for managing and installing plugins in Vim/Neovim. To install vim-plug, add the following to your ~/.vimrc or ~/.config/nvim/init.vim: call plug#begin('~/.vim/plugged') Plug 'tpope/vim-sensible' " Sensible default configurations Plug 'neoclide/coc.nvim' " Autocompletion and IntelliSense call plug#end() Plugins: coc.nvim: A comprehensive autocompletion plugin providing IntelliSense-like features. NERDTree: A file system explorer for Vim to easily navigate the project structure. 2. Version Control Systems 2.1 Git Overview: Git is a distributed version control system that enables teams to work collaboratively, maintain code history, and track changes efficiently. Configuration: Install Git: Download Git here. Basic Setup: git config --global user.name "Your Name" git config --global user.email "your_email@example.com" Plugins: GitLens: VS Code extension that supercharges Git capabilities with visualizations and history. SourceTree: A GUI for managing Git repositories, providing an intuitive interface for committing, branching, and merging. 3. Project Management Tools 3.1 Trello Overview: Trello is a visual project management tool that uses boards, lists, and cards to organize tasks. It is simple to use but highly effective for personal and team projects. Configuration: Set up Trello: Sign up here. Power-Ups: Enhance Trello boards with Power-Ups (integrations), such as: GitHub: Link GitHub repositories to Trello cards. Slack: Get Trello notifications in Slack channels. Plugins: Butler: Automate actions within Trello using simple commands (e.g., moving cards based on due dates). 3.2 Jira Overview: Jira is a comprehensive project management tool designed for software teams, often used in Agile environments to track sprints, issues, and progress. Configuration: Set up Jira: Jira Software provides cloud and self-hosted options. Integrations: Integrate Jira with GitHub, Bitbucket, or GitLab to track commits and branches. Plugins: Tempo Timesheets: Time tracking within Jira. Slack for Jira: Sync Jira with Slack to get notifications for tasks and issues. 4. CI/CD Tools 4.1 Jenkins Overview: Jenkins is an open-source automation server for building, deploying, and automating software projects. Configuration: Install Jenkins: Follow installation instructions from the Jenkins website. Pipeline Configuration: Create a Jenkinsfile to define a continuous integration pipeline. Plugins: Git Plugin: Allows Jenkins to integrate with Git repositories. Docker Pipeline Plugin: Integrates Docker with Jenkins pipelines, allowing Docker containers to be used as build agents. 4.2 GitHub Actions Overview: GitHub Actions is an automation tool that allows you to create custom workflows directly within GitHub repositories, enabling continuous integration and deployment. Configuration: Set up a GitHub Action: Create a .github/workflows/main.yml file with the workflow definition. Example: name: CI on: [push] jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Set up Node.js uses: actions/setup-node@v2 with: node-version: '14' - run: npm install Plugins: actions/setup-node: A popular action for setting up Node.js in CI pipelines. actions/cache: Cache dependencies to speed up builds. 5. Containerization &amp; Virtualization 5.1 Docker Overview: Docker allows developers to package applications and their dependencies into containers, ensuring consistent behavior across different environments. Configuration: Install Docker: Download Docker for your operating system. Basic Dockerfile: FROM node:14 WORKDIR /app COPY . . RUN npm install CMD ["npm", "start"] Plugins: Docker Compose: Tool to define and run multi-container Docker applications. Docker Extension for VS Code: Manage containers, images, and networks directly from the editor. 6. Time Management &amp; Focus 6.1 Pomodone Overview: Pomodone is a Pomodoro technique timer that helps developers break work into 25-minute intervals followed by short breaks. Configuration: Install Pomodone: Sign up here. Integrate with Tools: Sync tasks from Trello or Asana to Pomodone to manage your workflow effectively. Plugins: Trello Integration: Manage tasks from Trello boards and apply the Pomodoro technique. 7. Reading &amp; Learning 7.1 Bypass-paywalls-clean Overview: Bypass-paywalls-clean is a browser extension that allows you to access paywalled content for free. Configuration: Install Bypass-paywalls-clean: Download. Conclusion In this article, I have covered a wide range of productivity tools, from code editors and version control systems to project management and CI/CD tools. Each of these tools has its configuration methods and plugin extensions that can enhance the developer experience. By leveraging these tools and their respective plugins, developers can automate repetitive tasks, streamline their workflows, and ultimately improve productivity. This article serves as an introduction to some of the most effective productivity tools for developers. For detailed configurations and specific use cases, each tool’s official documentation and plugin resources should be consulted.]]></summary></entry><entry><title type="html">DevOps CI/CD GitLab</title><link href="http://localhost:4000/blog/devops-cicd-gitlab/" rel="alternate" type="text/html" title="DevOps CI/CD GitLab" /><published>2025-01-23T00:00:00+08:00</published><updated>2025-01-23T00:00:00+08:00</updated><id>http://localhost:4000/blog/devops-cicd-gitlab</id><content type="html" xml:base="http://localhost:4000/blog/devops-cicd-gitlab/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>GitLab is a powerful DevOps platform that provides a complete CI/CD pipeline for automating the build, test, and deployment of applications. In this guide, we’ll walk you through the steps to install <strong>GitLab</strong> on <strong>macOS</strong> using <strong>Docker</strong> and configure it for CI/CD workflows.</p>

<hr />

<h2 id="prerequisites">Prerequisites</h2>

<p>Before proceeding, ensure you have the following:</p>

<ul>
  <li><strong>macOS</strong> machine with an internet connection.</li>
  <li><strong>Docker</strong> installed on your system. If not, download it from <a href="https://www.docker.com/products/docker-desktop">Docker</a> and install it.</li>
  <li><strong>GitLab user credentials</strong> for accessing the GitLab UI.</li>
</ul>

<hr />

<h2 id="step-1-install-docker">Step 1: Install Docker</h2>

<p>First, ensure Docker is installed on your <strong>macOS</strong> system. If you haven’t done it yet, download Docker Desktop for macOS from the <a href="https://www.docker.com/products/docker-desktop">official website</a>. Once downloaded, follow the installation steps.</p>

<p>To verify the installation, run:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker <span class="nt">--version</span>
</code></pre></div></div>

<p>This should return the Docker version installed.</p>

<hr />

<h2 id="step-2-pull-the-gitlab-docker-image">Step 2: Pull the GitLab Docker Image</h2>

<p>To install <strong>GitLab</strong> using Docker, run the following command to pull the official GitLab Community Edition image:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker pull gitlab/gitlab-ce:latest
</code></pre></div></div>

<hr />

<h2 id="step-3-run-gitlab-using-docker">Step 3: Run GitLab Using Docker</h2>

<p>Once the image is downloaded, you can run GitLab in a Docker container. Execute the following command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run <span class="nt">-d</span> <span class="nt">-p</span> 80:80 <span class="nt">-p</span> 443:443 <span class="nt">-p</span> 22:22 <span class="nt">--name</span> gitlab <span class="nt">--restart</span> always <span class="se">\</span>
<span class="nt">-v</span> /your/host/path/to/gitlab/config:/etc/gitlab <span class="se">\</span>
<span class="nt">-v</span> /your/host/path/to/gitlab/data:/var/opt/gitlab <span class="se">\</span>
<span class="nt">-v</span> /your/host/path/to/gitlab/logs:/var/log/gitlab <span class="se">\</span>
gitlab/gitlab-ce:latest
</code></pre></div></div>

<h3 id="explanation">Explanation:</h3>
<ul>
  <li><code class="language-plaintext highlighter-rouge">-p 80:80 -p 443:443 -p 22:22</code>: Maps the required ports for HTTP, HTTPS, and SSH.</li>
  <li><code class="language-plaintext highlighter-rouge">--name gitlab</code>: Names the container <code class="language-plaintext highlighter-rouge">gitlab</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">--restart always</code>: Ensures GitLab restarts automatically if the container or Docker daemon restarts.</li>
  <li><code class="language-plaintext highlighter-rouge">-v /your/host/path</code>: Mounts host directories to the container for persistent data.</li>
</ul>

<p>Replace <code class="language-plaintext highlighter-rouge">/your/host/path</code> with the actual paths on your machine where GitLab data, logs, and configuration will be stored.</p>

<hr />

<h2 id="step-4-access-gitlab-web-interface">Step 4: Access GitLab Web Interface</h2>

<p>Once GitLab is running, open your web browser and navigate to:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>http://localhost
</code></pre></div></div>

<p>You should see the GitLab setup page. Follow the instructions to set up an admin password and configure your GitLab instance.</p>

<hr />

<h2 id="step-5-configure-gitlab-for-cicd">Step 5: Configure GitLab for CI/CD</h2>

<p>GitLab offers built-in CI/CD features to automate the build, test, and deployment of your code. To configure GitLab CI/CD:</p>

<ol>
  <li><strong>Create a <code class="language-plaintext highlighter-rouge">.gitlab-ci.yml</code> file</strong> in the root of your repository. This file defines the CI/CD pipeline for your project.</li>
</ol>

<p>Example <code class="language-plaintext highlighter-rouge">.gitlab-ci.yml</code> for a simple Node.js application:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">stages</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">build</span>
  <span class="pi">-</span> <span class="s">test</span>
  <span class="pi">-</span> <span class="s">deploy</span>

<span class="na">build</span><span class="pi">:</span>
  <span class="na">stage</span><span class="pi">:</span> <span class="s">build</span>
  <span class="na">script</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">npm install</span>

<span class="na">test</span><span class="pi">:</span>
  <span class="na">stage</span><span class="pi">:</span> <span class="s">test</span>
  <span class="na">script</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">npm test</span>

<span class="na">deploy</span><span class="pi">:</span>
  <span class="na">stage</span><span class="pi">:</span> <span class="s">deploy</span>
  <span class="na">script</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">./deploy.sh</span>
</code></pre></div></div>

<ol>
  <li><strong>Push your repository to GitLab</strong>. GitLab will automatically detect the <code class="language-plaintext highlighter-rouge">.gitlab-ci.yml</code> file and trigger the pipeline based on the defined stages.</li>
</ol>

<hr />

<h2 id="step-6-monitor-gitlab-logs">Step 6: Monitor GitLab Logs</h2>

<p>To monitor the logs of the GitLab container, you can use the following command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker logs <span class="nt">-f</span> gitlab
</code></pre></div></div>

<p>This will show real-time logs for the GitLab container.</p>

<hr />

<h2 id="step-7-automating-gitlab-restart-optional">Step 7: Automating GitLab Restart (Optional)</h2>

<p>To ensure GitLab restarts automatically after a system reboot, you can set up Docker’s restart policy. Run the following command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker update <span class="nt">--restart</span> unless-stopped gitlab
</code></pre></div></div>

<hr />

<h2 id="step-8-backup-gitlab-optional">Step 8: Backup GitLab (Optional)</h2>

<p>It’s important to back up your GitLab data regularly. Follow these steps to back it up:</p>

<ol>
  <li>
    <p><strong>Stop GitLab container</strong>:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker stop gitlab
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Backup GitLab data</strong>:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker <span class="nb">cp </span>gitlab:/var/opt/gitlab /path/to/gitlab/backup/folder
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Restart GitLab container</strong>:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker start gitlab
</code></pre></div>    </div>
  </li>
</ol>

<hr />

<h2 id="conclusion">Conclusion</h2>

<p>By following this guide, you have successfully installed <strong>GitLab</strong> on <strong>macOS</strong> using Docker and set up <strong>CI/CD pipelines</strong> for your projects. GitLab makes it easy to automate your build, test, and deployment processes.</p>

<p>Let me know if you need further assistance!</p>]]></content><author><name>Madden Zhang</name></author><category term="Blog" /><category term="devOps" /><category term="gitlab" /><category term="CI/CD" /><summary type="html"><![CDATA[Introduction GitLab is a powerful DevOps platform that provides a complete CI/CD pipeline for automating the build, test, and deployment of applications. In this guide, we’ll walk you through the steps to install GitLab on macOS using Docker and configure it for CI/CD workflows. Prerequisites Before proceeding, ensure you have the following: macOS machine with an internet connection. Docker installed on your system. If not, download it from Docker and install it. GitLab user credentials for accessing the GitLab UI. Step 1: Install Docker First, ensure Docker is installed on your macOS system. If you haven’t done it yet, download Docker Desktop for macOS from the official website. Once downloaded, follow the installation steps. To verify the installation, run: docker --version This should return the Docker version installed. Step 2: Pull the GitLab Docker Image To install GitLab using Docker, run the following command to pull the official GitLab Community Edition image: docker pull gitlab/gitlab-ce:latest Step 3: Run GitLab Using Docker Once the image is downloaded, you can run GitLab in a Docker container. Execute the following command: docker run -d -p 80:80 -p 443:443 -p 22:22 --name gitlab --restart always \ -v /your/host/path/to/gitlab/config:/etc/gitlab \ -v /your/host/path/to/gitlab/data:/var/opt/gitlab \ -v /your/host/path/to/gitlab/logs:/var/log/gitlab \ gitlab/gitlab-ce:latest Explanation: -p 80:80 -p 443:443 -p 22:22: Maps the required ports for HTTP, HTTPS, and SSH. --name gitlab: Names the container gitlab. --restart always: Ensures GitLab restarts automatically if the container or Docker daemon restarts. -v /your/host/path: Mounts host directories to the container for persistent data. Replace /your/host/path with the actual paths on your machine where GitLab data, logs, and configuration will be stored. Step 4: Access GitLab Web Interface Once GitLab is running, open your web browser and navigate to: http://localhost You should see the GitLab setup page. Follow the instructions to set up an admin password and configure your GitLab instance. Step 5: Configure GitLab for CI/CD GitLab offers built-in CI/CD features to automate the build, test, and deployment of your code. To configure GitLab CI/CD: Create a .gitlab-ci.yml file in the root of your repository. This file defines the CI/CD pipeline for your project. Example .gitlab-ci.yml for a simple Node.js application: stages: - build - test - deploy build: stage: build script: - npm install test: stage: test script: - npm test deploy: stage: deploy script: - ./deploy.sh Push your repository to GitLab. GitLab will automatically detect the .gitlab-ci.yml file and trigger the pipeline based on the defined stages. Step 6: Monitor GitLab Logs To monitor the logs of the GitLab container, you can use the following command: docker logs -f gitlab This will show real-time logs for the GitLab container. Step 7: Automating GitLab Restart (Optional) To ensure GitLab restarts automatically after a system reboot, you can set up Docker’s restart policy. Run the following command: docker update --restart unless-stopped gitlab Step 8: Backup GitLab (Optional) It’s important to back up your GitLab data regularly. Follow these steps to back it up: Stop GitLab container: docker stop gitlab Backup GitLab data: docker cp gitlab:/var/opt/gitlab /path/to/gitlab/backup/folder Restart GitLab container: docker start gitlab Conclusion By following this guide, you have successfully installed GitLab on macOS using Docker and set up CI/CD pipelines for your projects. GitLab makes it easy to automate your build, test, and deployment processes. Let me know if you need further assistance!]]></summary></entry><entry><title type="html">Devops CI/CD Jenkins</title><link href="http://localhost:4000/blog/devops-cicd-jenkins/" rel="alternate" type="text/html" title="Devops CI/CD Jenkins" /><published>2025-01-23T00:00:00+08:00</published><updated>2025-01-23T00:00:00+08:00</updated><id>http://localhost:4000/blog/devops-cicd-jenkins</id><content type="html" xml:base="http://localhost:4000/blog/devops-cicd-jenkins/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>Jenkins is a popular open-source automation server that facilitates continuous integration and continuous delivery (CI/CD). It helps automate the building, testing, and deployment of applications. This guide provides detailed steps to install Jenkins on <strong>macOS</strong> using <strong>Docker</strong>, configure a basic pipeline, and introduce approval processes for SQL script execution and release deployment.</p>

<p>Using Docker simplifies the installation process and ensures that Jenkins is isolated in a container, avoiding dependency conflicts with the host system. After installation, we will configure Jenkins for building, testing, and deploying applications while introducing a simple approval process.</p>

<hr />

<h2 id="prerequisites">Prerequisites</h2>

<p>Before proceeding, ensure you have the following:</p>

<ul>
  <li><strong>macOS</strong> machine with an internet connection.</li>
  <li><strong>Docker</strong> installed on your system. If not, download it from <a href="https://www.docker.com/products/docker-desktop">Docker</a> and install it.</li>
  <li><strong>Jenkins user credentials</strong> for logging into the Jenkins UI.</li>
</ul>

<hr />

<h2 id="step-1-install-docker">Step 1: Install Docker</h2>

<p>First, ensure Docker is installed on your <strong>macOS</strong> system. If you haven’t done it yet, download Docker Desktop for macOS from the <a href="https://www.docker.com/products/docker-desktop">official website</a>. Once downloaded, follow the installation steps.</p>

<p>To verify the installation, run:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker <span class="nt">--version</span>
</code></pre></div></div>

<p>This should return the Docker version installed.</p>

<hr />

<h2 id="step-2-pull-the-jenkins-docker-image">Step 2: Pull the Jenkins Docker Image</h2>

<p>Now that Docker is installed, we can pull the official Jenkins image. Open your terminal and run the following command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker pull jenkins/jenkins:lts
</code></pre></div></div>

<p>This will pull the <strong>Long-Term Support (LTS)</strong> version of Jenkins, which is stable and recommended for production environments.</p>

<hr />

<h2 id="step-3-run-jenkins-using-docker">Step 3: Run Jenkins Using Docker</h2>

<p>Once the image is downloaded, you can run Jenkins in a Docker container. Execute the following command to run Jenkins:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run <span class="nt">-d</span> <span class="nt">-p</span> 8080:8080 <span class="nt">-p</span> 50000:50000 <span class="nt">--name</span> jenkins jenkins/jenkins:lts
</code></pre></div></div>

<h3 id="explanation">Explanation:</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">-d</code>: Run the container in detached mode.</li>
  <li><code class="language-plaintext highlighter-rouge">-p 8080:8080</code>: Map port <code class="language-plaintext highlighter-rouge">8080</code> on the container to port <code class="language-plaintext highlighter-rouge">8080</code> on your host, which allows you to access Jenkins UI at <code class="language-plaintext highlighter-rouge">http://localhost:8080</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">-p 50000:50000</code>: Expose the port <code class="language-plaintext highlighter-rouge">50000</code> for communication between Jenkins and its agents.</li>
  <li><code class="language-plaintext highlighter-rouge">--name jenkins</code>: Name the container <code class="language-plaintext highlighter-rouge">jenkins</code> for easy reference.</li>
</ul>

<hr />

<h2 id="step-4-access-jenkins-web-interface">Step 4: Access Jenkins Web Interface</h2>

<p>To access Jenkins, open your web browser and navigate to:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>http://localhost:8080
</code></pre></div></div>

<p>On your first visit, Jenkins will ask for an <strong>unlock key</strong>. To retrieve it, execute the following command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker <span class="nb">exec</span> <span class="nt">-it</span> jenkins <span class="nb">cat</span> /var/jenkins_home/secrets/initialAdminPassword
</code></pre></div></div>

<p>Copy the output (the password), and paste it into the Jenkins unlock screen to proceed with the initial setup.</p>

<hr />

<h2 id="step-5-set-up-jenkins">Step 5: Set Up Jenkins</h2>

<p>Once Jenkins is unlocked, follow the setup wizard:</p>

<ol>
  <li><strong>Install suggested plugins</strong>: Jenkins will suggest a set of commonly used plugins. Click on “Install suggested plugins.”</li>
  <li><strong>Create an admin user</strong>: After plugins are installed, create a user with admin privileges by following the prompts. This is your primary user for accessing Jenkins.</li>
  <li><strong>Jenkins Setup Complete</strong>: After creating the admin user, Jenkins will show a “Setup is Complete” message.</li>
</ol>

<hr />

<h2 id="step-6-configure-jenkins-pipeline-with-sql-and-release-approval">Step 6: Configure Jenkins Pipeline with SQL and Release Approval</h2>

<p>Jenkins allows you to automate tasks through <strong>Pipelines</strong>, which define your build, test, and deployment workflows. To get started, we’ll configure a simple pipeline and introduce an <strong>approval process</strong> before executing SQL scripts and releasing deployments.</p>

<h3 id="a-install-required-plugins">a) Install Required Plugins</h3>

<p>To enable pipeline support and approval steps, you need the following plugins:</p>

<ol>
  <li><strong>Pipeline</strong>: For creating Jenkins pipelines.</li>
  <li><strong>Input Step Plugin</strong>: To introduce manual approval in the pipeline.</li>
</ol>

<p>To install these:</p>

<ol>
  <li>Go to <code class="language-plaintext highlighter-rouge">Manage Jenkins</code> &gt; <code class="language-plaintext highlighter-rouge">Manage Plugins</code>.</li>
  <li>Under the <strong>Available</strong> tab, search and install the <strong>Pipeline</strong> and <strong>Input Step Plugin</strong>.</li>
</ol>

<h3 id="b-create-a-new-pipeline">b) Create a New Pipeline</h3>

<ol>
  <li>On the Jenkins dashboard, click on <strong>New Item</strong>.</li>
  <li>Enter a name for your project (e.g., “MyFirstPipeline”).</li>
  <li>Choose <strong>Pipeline</strong> and click <strong>OK</strong>.</li>
  <li>In the configuration screen, scroll down to the <strong>Pipeline</strong> section and enter the following script:</li>
</ol>

<div class="language-groovy highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pipeline</span> <span class="o">{</span>
    <span class="n">agent</span> <span class="n">any</span>

    <span class="n">environment</span> <span class="o">{</span>
        <span class="c1">// 这里可以设置一些全局变量或常用参数，如数据库连接等</span>
        <span class="n">DB_HOST</span> <span class="o">=</span> <span class="s1">'your_db_host'</span>
        <span class="n">DB_USER</span> <span class="o">=</span> <span class="s1">'your_db_user'</span>
        <span class="n">DB_PASS</span> <span class="o">=</span> <span class="s1">'your_db_password'</span>
    <span class="o">}</span>

    <span class="n">stages</span> <span class="o">{</span>
        <span class="n">stage</span><span class="o">(</span><span class="s1">'Checkout'</span><span class="o">)</span> <span class="o">{</span>
            <span class="n">steps</span> <span class="o">{</span>
                <span class="c1">// 检出代码库，假设你使用Git</span>
                <span class="n">checkout</span> <span class="n">scm</span>
            <span class="o">}</span>
        <span class="o">}</span>

        <span class="n">stage</span><span class="o">(</span><span class="s1">'SQL Approval'</span><span class="o">)</span> <span class="o">{</span>
            <span class="n">steps</span> <span class="o">{</span>
                <span class="c1">// 在SQL审批阶段进行人工审批，审批人员可以填写SQL审批意见</span>
                <span class="n">script</span> <span class="o">{</span>
                    <span class="kt">def</span> <span class="n">approval</span> <span class="o">=</span> <span class="n">input</span> <span class="nl">message:</span> <span class="s1">'SQL脚本审批'</span><span class="o">,</span> 
                                          <span class="nl">parameters:</span> <span class="o">[</span>
                                              <span class="n">string</span><span class="o">(</span><span class="nl">defaultValue:</span> <span class="s1">''</span><span class="o">,</span> <span class="nl">description:</span> <span class="s1">'请输入SQL审批意见'</span><span class="o">,</span> <span class="nl">name:</span> <span class="s1">'sql_approval_comments'</span><span class="o">)</span>
                                          <span class="o">]</span>
                    <span class="c1">// 将审批意见存储或使用</span>
                    <span class="n">echo</span> <span class="s2">"SQL审批意见: ${approval['sql_approval_comments']}"</span>
                <span class="o">}</span>
            <span class="o">}</span>
        <span class="o">}</span>

        <span class="n">stage</span><span class="o">(</span><span class="s1">'Run SQL Scripts'</span><span class="o">)</span> <span class="o">{</span>
            <span class="n">steps</span> <span class="o">{</span>
                <span class="c1">// 在SQL审批通过后执行SQL脚本</span>
                <span class="n">script</span> <span class="o">{</span>
                    <span class="n">echo</span> <span class="s1">'执行SQL脚本...'</span>
                    <span class="c1">// 这里假设你用sh步骤来执行SQL脚本，也可以使用数据库相关插件</span>
                    <span class="n">sh</span> <span class="s1">'''
                    mysql -h ${DB_HOST} -u ${DB_USER} -p${DB_PASS} -e "source ./your_sql_script.sql"
                    '''</span>
                <span class="o">}</span>
            <span class="o">}</span>
        <span class="o">}</span>

        <span class="n">stage</span><span class="o">(</span><span class="s1">'Release Approval'</span><span class="o">)</span> <span class="o">{</span>
            <span class="n">steps</span> <span class="o">{</span>
                <span class="c1">// 在上线前进行审批，审批人可以填写上线内容</span>
                <span class="n">script</span> <span class="o">{</span>
                    <span class="kt">def</span> <span class="n">releaseApproval</span> <span class="o">=</span> <span class="n">input</span> <span class="nl">message:</span> <span class="s1">'是否批准上线？'</span><span class="o">,</span>
                                                 <span class="nl">parameters:</span> <span class="o">[</span>
                                                     <span class="n">string</span><span class="o">(</span><span class="nl">defaultValue:</span> <span class="s1">''</span><span class="o">,</span> <span class="nl">description:</span> <span class="s1">'请输入上线内容'</span><span class="o">,</span> <span class="nl">name:</span> <span class="s1">'release_notes'</span><span class="o">),</span>
                                                     <span class="n">string</span><span class="o">(</span><span class="nl">defaultValue:</span> <span class="s1">''</span><span class="o">,</span> <span class="nl">description:</span> <span class="s1">'请输入上线审批意见'</span><span class="o">,</span> <span class="nl">name:</span> <span class="s1">'release_approval_comments'</span><span class="o">)</span>
                                                 <span class="o">]</span>
                    <span class="c1">// 获取上线内容和审批意见</span>
                    <span class="n">echo</span> <span class="s2">"上线内容: ${releaseApproval['release_notes']}"</span>
                    <span class="n">echo</span> <span class="s2">"上线审批意见: ${releaseApproval['release_approval_comments']}"</span>
                <span class="o">}</span>
            <span class="o">}</span>
        <span class="o">}</span>

        <span class="n">stage</span><span class="o">(</span><span class="s1">'Deploy'</span><span class="o">)</span> <span class="o">{</span>
            <span class="n">steps</span> <span class="o">{</span>
                <span class="c1">// 上线操作，假设部署应用</span>
                <span class="n">script</span> <span class="o">{</span>
                    <span class="n">echo</span> <span class="s1">'执行部署操作...'</span>
                    <span class="n">sh</span> <span class="s1">'''
                    ./deploy.sh  # 替换为实际的部署脚本或命令
                    '''</span>
                <span class="o">}</span>
            <span class="o">}</span>
        <span class="o">}</span>
    <span class="o">}</span>

    <span class="n">post</span> <span class="o">{</span>
        <span class="n">always</span> <span class="o">{</span>
            <span class="c1">// 这里可以定义清理工作或通知等</span>
            <span class="n">echo</span> <span class="s1">'Pipeline 执行完毕'</span>
        <span class="o">}</span>

        <span class="n">success</span> <span class="o">{</span>
            <span class="n">echo</span> <span class="s1">'Pipeline 执行成功'</span>
        <span class="o">}</span>

        <span class="n">failure</span> <span class="o">{</span>
            <span class="n">echo</span> <span class="s1">'Pipeline 执行失败'</span>
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<h3 id="c-save-and-run-the-pipeline">c) Save and Run the Pipeline</h3>

<ul>
  <li>After entering the pipeline script, click <strong>Save</strong>.</li>
  <li>Trigger the pipeline by clicking <strong>Build Now</strong>.</li>
</ul>

<p>The pipeline will:</p>
<ol>
  <li><strong>Checkout</strong> the code from your version control system (assuming it’s linked).</li>
  <li><strong>SQL Approval</strong>: Wait for manual approval and SQL comments before executing SQL scripts.</li>
  <li><strong>Run SQL Scripts</strong>: Execute SQL scripts only after SQL approval.</li>
  <li><strong>Release Approval</strong>: Wait for manual approval of the release, where the release notes and approval comments are entered.</li>
  <li><strong>Deploy</strong>: Once all approvals are granted, the deployment will proceed.</li>
</ol>

<hr />

<h2 id="step-7-monitor-jenkins-logs-and-jobs">Step 7: Monitor Jenkins Logs and Jobs</h2>

<p>To monitor Jenkins logs and ensure the server is running smoothly, use the following command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker logs <span class="nt">-f</span> jenkins
</code></pre></div></div>

<p>This will display Jenkins logs in real-time. To view the build job logs, go to the <strong>Build History</strong> section in your Jenkins project page.</p>

<hr />

<h2 id="step-8-automating-jenkins-restart-optional">Step 8: Automating Jenkins Restart (Optional)</h2>

<p>To ensure that Jenkins restarts automatically after a system reboot, you can set up Docker’s restart policy. Stop the container and run the following command to restart it automatically on failure or reboot:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker update <span class="nt">--restart</span> unless-stopped jenkins
</code></pre></div></div>

<hr />

<h2 id="step-9-configure-backups-optional">Step 9: Configure Backups (Optional)</h2>

<p>It’s important to regularly back up your Jenkins data. To back up your Jenkins instance and its configuration, follow these steps:</p>

<ol>
  <li>
    <p><strong>Stop the Jenkins container</strong>:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker stop jenkins
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Create a backup of Jenkins data</strong>:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker <span class="nb">cp </span>jenkins:/var/jenkins_home /path/to/backup/folder
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Restart the Jenkins container</strong>:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker start jenkins
</code></pre></div>    </div>
  </li>
</ol>

<hr />

<h2 id="conclusion">Conclusion</h2>

<p>By following this guide, you have successfully installed Jenkins on your <strong>macOS</strong> using Docker, set up a basic <strong>CI/CD pipeline</strong> with SQL and release approval processes. You can now automate your builds, tests, and deployments, and track the approval steps</p>]]></content><author><name>Madden Zhang</name></author><category term="Blog" /><category term="devOps" /><category term="jenkins" /><category term="CI/CD" /><summary type="html"><![CDATA[Introduction Jenkins is a popular open-source automation server that facilitates continuous integration and continuous delivery (CI/CD). It helps automate the building, testing, and deployment of applications. This guide provides detailed steps to install Jenkins on macOS using Docker, configure a basic pipeline, and introduce approval processes for SQL script execution and release deployment. Using Docker simplifies the installation process and ensures that Jenkins is isolated in a container, avoiding dependency conflicts with the host system. After installation, we will configure Jenkins for building, testing, and deploying applications while introducing a simple approval process. Prerequisites Before proceeding, ensure you have the following: macOS machine with an internet connection. Docker installed on your system. If not, download it from Docker and install it. Jenkins user credentials for logging into the Jenkins UI. Step 1: Install Docker First, ensure Docker is installed on your macOS system. If you haven’t done it yet, download Docker Desktop for macOS from the official website. Once downloaded, follow the installation steps. To verify the installation, run: docker --version This should return the Docker version installed. Step 2: Pull the Jenkins Docker Image Now that Docker is installed, we can pull the official Jenkins image. Open your terminal and run the following command: docker pull jenkins/jenkins:lts This will pull the Long-Term Support (LTS) version of Jenkins, which is stable and recommended for production environments. Step 3: Run Jenkins Using Docker Once the image is downloaded, you can run Jenkins in a Docker container. Execute the following command to run Jenkins: docker run -d -p 8080:8080 -p 50000:50000 --name jenkins jenkins/jenkins:lts Explanation: -d: Run the container in detached mode. -p 8080:8080: Map port 8080 on the container to port 8080 on your host, which allows you to access Jenkins UI at http://localhost:8080. -p 50000:50000: Expose the port 50000 for communication between Jenkins and its agents. --name jenkins: Name the container jenkins for easy reference. Step 4: Access Jenkins Web Interface To access Jenkins, open your web browser and navigate to: http://localhost:8080 On your first visit, Jenkins will ask for an unlock key. To retrieve it, execute the following command: docker exec -it jenkins cat /var/jenkins_home/secrets/initialAdminPassword Copy the output (the password), and paste it into the Jenkins unlock screen to proceed with the initial setup. Step 5: Set Up Jenkins Once Jenkins is unlocked, follow the setup wizard: Install suggested plugins: Jenkins will suggest a set of commonly used plugins. Click on “Install suggested plugins.” Create an admin user: After plugins are installed, create a user with admin privileges by following the prompts. This is your primary user for accessing Jenkins. Jenkins Setup Complete: After creating the admin user, Jenkins will show a “Setup is Complete” message. Step 6: Configure Jenkins Pipeline with SQL and Release Approval Jenkins allows you to automate tasks through Pipelines, which define your build, test, and deployment workflows. To get started, we’ll configure a simple pipeline and introduce an approval process before executing SQL scripts and releasing deployments. a) Install Required Plugins To enable pipeline support and approval steps, you need the following plugins: Pipeline: For creating Jenkins pipelines. Input Step Plugin: To introduce manual approval in the pipeline. To install these: Go to Manage Jenkins &gt; Manage Plugins. Under the Available tab, search and install the Pipeline and Input Step Plugin. b) Create a New Pipeline On the Jenkins dashboard, click on New Item. Enter a name for your project (e.g., “MyFirstPipeline”). Choose Pipeline and click OK. In the configuration screen, scroll down to the Pipeline section and enter the following script: pipeline { agent any environment { // 这里可以设置一些全局变量或常用参数，如数据库连接等 DB_HOST = 'your_db_host' DB_USER = 'your_db_user' DB_PASS = 'your_db_password' } stages { stage('Checkout') { steps { // 检出代码库，假设你使用Git checkout scm } } stage('SQL Approval') { steps { // 在SQL审批阶段进行人工审批，审批人员可以填写SQL审批意见 script { def approval = input message: 'SQL脚本审批', parameters: [ string(defaultValue: '', description: '请输入SQL审批意见', name: 'sql_approval_comments') ] // 将审批意见存储或使用 echo "SQL审批意见: ${approval['sql_approval_comments']}" } } } stage('Run SQL Scripts') { steps { // 在SQL审批通过后执行SQL脚本 script { echo '执行SQL脚本...' // 这里假设你用sh步骤来执行SQL脚本，也可以使用数据库相关插件 sh ''' mysql -h ${DB_HOST} -u ${DB_USER} -p${DB_PASS} -e "source ./your_sql_script.sql" ''' } } } stage('Release Approval') { steps { // 在上线前进行审批，审批人可以填写上线内容 script { def releaseApproval = input message: '是否批准上线？', parameters: [ string(defaultValue: '', description: '请输入上线内容', name: 'release_notes'), string(defaultValue: '', description: '请输入上线审批意见', name: 'release_approval_comments') ] // 获取上线内容和审批意见 echo "上线内容: ${releaseApproval['release_notes']}" echo "上线审批意见: ${releaseApproval['release_approval_comments']}" } } } stage('Deploy') { steps { // 上线操作，假设部署应用 script { echo '执行部署操作...' sh ''' ./deploy.sh # 替换为实际的部署脚本或命令 ''' } } } } post { always { // 这里可以定义清理工作或通知等 echo 'Pipeline 执行完毕' } success { echo 'Pipeline 执行成功' } failure { echo 'Pipeline 执行失败' } } } c) Save and Run the Pipeline After entering the pipeline script, click Save. Trigger the pipeline by clicking Build Now. The pipeline will: Checkout the code from your version control system (assuming it’s linked). SQL Approval: Wait for manual approval and SQL comments before executing SQL scripts. Run SQL Scripts: Execute SQL scripts only after SQL approval. Release Approval: Wait for manual approval of the release, where the release notes and approval comments are entered. Deploy: Once all approvals are granted, the deployment will proceed. Step 7: Monitor Jenkins Logs and Jobs To monitor Jenkins logs and ensure the server is running smoothly, use the following command: docker logs -f jenkins This will display Jenkins logs in real-time. To view the build job logs, go to the Build History section in your Jenkins project page. Step 8: Automating Jenkins Restart (Optional) To ensure that Jenkins restarts automatically after a system reboot, you can set up Docker’s restart policy. Stop the container and run the following command to restart it automatically on failure or reboot: docker update --restart unless-stopped jenkins Step 9: Configure Backups (Optional) It’s important to regularly back up your Jenkins data. To back up your Jenkins instance and its configuration, follow these steps: Stop the Jenkins container: docker stop jenkins Create a backup of Jenkins data: docker cp jenkins:/var/jenkins_home /path/to/backup/folder Restart the Jenkins container: docker start jenkins Conclusion By following this guide, you have successfully installed Jenkins on your macOS using Docker, set up a basic CI/CD pipeline with SQL and release approval processes. You can now automate your builds, tests, and deployments, and track the approval steps]]></summary></entry><entry><title type="html">Migration ILOG Solutions</title><link href="http://localhost:4000/blog/migration-ilog-solutions/" rel="alternate" type="text/html" title="Migration ILOG Solutions" /><published>2025-01-22T00:00:00+08:00</published><updated>2025-01-22T00:00:00+08:00</updated><id>http://localhost:4000/blog/migration-ilog-solutions</id><content type="html" xml:base="http://localhost:4000/blog/migration-ilog-solutions/"><![CDATA[<h3 id="detailed-plans-and-comparison-of-three-rule-migration-approaches">Detailed Plans and Comparison of Three Rule Migration Approaches</h3>

<p>This document outlines the detailed plans for three approaches to rule migration: using AI-assisted model transformation, using ANTLR for model transformation, and using manual methods combined with Python scripts for model transformation. Additionally, it provides a comparative analysis of these methods across various aspects.</p>

<hr />

<h4 id="1-ai-assisted-model-transformation">1. AI-Assisted Model Transformation</h4>

<p><strong>Description:</strong></p>

<p>This approach utilizes large language models (such as ChatGPT or T5) to transform rule or model files. These models are adept at handling structured or semi-structured data and are particularly suited for semantic mapping between different rule formats. The migration process involves collecting the original rule data (e.g., ILOG rules), extracting common patterns or templates, analyzing the target rule format requirements, and forming a comprehensive transformation map to allow the model to understand and perform the conversion.</p>

<ul>
  <li>
    <p><strong>Testing with Prompt Engineering:</strong>
This method designs prompts tailored to large models. For instance, one can craft specific prompts to guide the model in understanding the structure and semantic mapping of rules.</p>
  </li>
  <li>
    <p><strong>Testing with Fine-tuning:</strong>
In this approach, the existing dataset is used to fine-tune the large language model. The model learns through parameter adjustments, thereby achieving improved performance for the task.</p>
  </li>
</ul>

<p><strong>Advantages and Disadvantages:</strong></p>

<ul>
  <li><strong>Advantages:</strong>
    <ul>
      <li>Rapid processing of large numbers of rules, especially when rules follow fixed patterns.</li>
      <li>No in-depth understanding of rule syntax is required, as long as reasonable prompts are designed.</li>
    </ul>
  </li>
  <li><strong>Disadvantages:</strong>
    <ul>
      <li>The model’s ability to handle complex logic is limited, and manual corrections may be necessary.</li>
      <li>When dealing with high-complexity rules, the model may make errors. Additionally, the fine-tuning effect is difficult to control.</li>
    </ul>
  </li>
</ul>

<hr />

<h4 id="2-using-antlr-for-model-transformation">2. Using ANTLR for Model Transformation</h4>

<p><strong>Description:</strong></p>

<p>ANTLR (Another Tool for Language Recognition) is a powerful syntax parsing tool that allows custom parsers to transform rule languages from one format to another. This approach is suitable for scenarios that require high precision and control over the parsing and transformation process.</p>

<ul>
  <li><strong>Implementation Steps:</strong>
    <ol>
      <li>
        <p><strong>Define Rule Syntax:</strong>
Design syntax files for both the original and target rule languages.</p>
      </li>
      <li>
        <p><strong>Generate Parser:</strong>
Use the ANTLR tool to generate parsers (supporting languages like Java, Python, etc.).</p>
      </li>
      <li>
        <p><strong>Write Conversion Logic:</strong>
Implement the rule conversion logic using the generated parsers.</p>
      </li>
    </ol>
  </li>
</ul>

<p><strong>Advantages and Disadvantages:</strong></p>

<ul>
  <li><strong>Advantages:</strong>
    <ul>
      <li>High flexibility, allowing for fully customized parsing and transformation rules, making it suitable for complex scenarios.</li>
      <li>High precision in parsing and conversion, ideal for migrating complex logic.</li>
      <li>Strong scalability, with the ability to continuously adjust the parser to accommodate new rules.</li>
    </ul>
  </li>
  <li><strong>Disadvantages:</strong>
    <ul>
      <li>A long development cycle, requiring deep understanding of both the original and target rule syntax.</li>
      <li>High learning curve for teams unfamiliar with the grammar.</li>
      <li>Initial development costs are significant, and maintaining the custom parsers requires expertise.</li>
    </ul>
  </li>
</ul>

<hr />

<h4 id="3-manual-and-python-script-based-model-transformation">3. Manual and Python Script-Based Model Transformation</h4>

<p><strong>Description:</strong></p>

<p>This approach involves manually analyzing the rule file structure and writing Python scripts to transform the rules from one format to another. It is most effective for scenarios where the number of rules is small, and the structure is relatively simple.</p>

<ul>
  <li><strong>Implementation Steps:</strong>
    <ol>
      <li>
        <p><strong>Rule Analysis:</strong>
Manually analyze the original rule file format and document the structural differences between the original and target rule formats.</p>
      </li>
      <li>
        <p><strong>Write Script:</strong>
Use Python (combined with regular expressions or basic parsing libraries) to write rule extraction and transformation scripts.</p>

        <p>Example:</p>
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">re</span>

<span class="k">def</span> <span class="nf">convert_rule</span><span class="p">(</span><span class="n">input_rule</span><span class="p">):</span>
    <span class="n">match</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">match</span><span class="p">(</span><span class="sa">r</span><span class="s">'if (.+) then (.+);'</span><span class="p">,</span> <span class="n">input_rule</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">match</span><span class="p">:</span>
        <span class="n">condition</span><span class="p">,</span> <span class="n">action</span> <span class="o">=</span> <span class="n">match</span><span class="p">.</span><span class="n">groups</span><span class="p">()</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s">'rule "</span><span class="si">{</span><span class="n">action</span><span class="p">.</span><span class="n">strip</span><span class="p">()</span><span class="si">}</span><span class="s">" when </span><span class="si">{</span><span class="n">condition</span><span class="si">}</span><span class="s"> then </span><span class="si">{</span><span class="n">action</span><span class="si">}</span><span class="s">;'</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p><strong>Batch Processing:</strong>
Feed the rule files into the script in bulk and output the converted target rule files.</p>
      </li>
      <li>
        <p><strong>Manual Verification:</strong>
Perform manual checks on the converted results and address any inconsistencies.</p>
      </li>
    </ol>
  </li>
</ul>

<p><strong>Advantages and Disadvantages:</strong></p>

<ul>
  <li><strong>Advantages:</strong>
    <ul>
      <li>Simple implementation with low learning costs, suitable for small-scale projects.</li>
      <li>High flexibility, allowing rapid adjustments to the script based on rule variations.</li>
      <li>Low initial cost, requiring no additional tools or resources.</li>
    </ul>
  </li>
  <li><strong>Disadvantages:</strong>
    <ul>
      <li>Limited ability to handle complex rule logic, with inefficient processing for intricate rules.</li>
      <li>Manual analysis and validation are time-consuming, limiting the scalability of this approach.</li>
      <li>Conversion accuracy is highly dependent on script quality, and high manual intervention may be required.</li>
    </ul>
  </li>
</ul>

<hr />

<h4 id="4-comparative-analysis">4. Comparative Analysis</h4>

<p><strong>4.1 Suitable Scenarios Comparison:</strong></p>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>Suitable Scenarios</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>AI-Assisted Migration</strong></td>
      <td>Scenarios where rule patterns are clear and semantic consistency is high. Suitable for natural language rule conversions or situations where existing large models can be leveraged.</td>
    </tr>
    <tr>
      <td><strong>ANTLR Migration</strong></td>
      <td>High-complexity rules with strict syntax and precision requirements. Ideal for large-scale or long-term rule migration needs.</td>
    </tr>
    <tr>
      <td><strong>Manual and Python Script Migration</strong></td>
      <td>Small-scale projects with a low number of rules and simple rule structures. Best suited for one-time migrations or projects lacking specialized tools.</td>
    </tr>
  </tbody>
</table>

<hr />

<h4 id="5-solution-selection">5. Solution Selection</h4>

<ol>
  <li>
    <p><strong>AI-Assisted Migration:</strong>
Based on testing, prompt-based templates provide a high degree of understanding and accuracy for model conversions. However, ChatGPT, due to security concerns, requires API access or deployment via Microsoft’s independent solution. Open-source large models, when deployed locally, do not provide satisfactory results for prompt engineering and fine-tuning. We used T5-large (770M, 770 million parameters) as the model size; unfortunately, we cannot test models with 3 billion (3B) or 11 billion (11B) parameters locally due to hardware limitations. The GPT-4 model, with parameter counts between 200 billion to 300 billion, offers potential for more powerful results.</p>
  </li>
  <li>
    <p><strong>ANTLR Parser:</strong>
The ANTLR-based solution is promising, but requires a thorough understanding of ILOG’s syntax and lexicon to create a versatile tool capable of handling various scenarios. Developing a generalized tool may prove challenging.</p>
  </li>
  <li>
    <p><strong>Manual and Python Script Migration:</strong>
This approach can be swiftly implemented based on the current syntax and lexicon. It is effective for targeted rule mapping, using hardcoded approaches for continuous processing. Uncommon cases can be added incrementally, with verification and supplementation through manual intervention and scripting.</p>
  </li>
</ol>

<hr />

<h3 id="conclusion">Conclusion</h3>

<p>The choice of rule migration approach largely depends on the specific needs of the migration project, including the complexity of the rules, the scale of the migration, and the available resources. AI-assisted methods offer speed but may require significant refinement, ANTLR offers high precision but at the cost of complexity and longer development cycles, while manual and Python script-based approaches are ideal for smaller, simpler projects but may lack scalability.</p>

<p>Each method offers distinct advantages and trade-offs, and careful consideration of the specific use case is required to select the most appropriate solution for efficient and accurate rule migration.</p>

<hr />]]></content><author><name>Madden Zhang</name></author><category term="Blog" /><category term="migration" /><category term="solutions" /><category term="ILOG" /><summary type="html"><![CDATA[Detailed Plans and Comparison of Three Rule Migration Approaches This document outlines the detailed plans for three approaches to rule migration: using AI-assisted model transformation, using ANTLR for model transformation, and using manual methods combined with Python scripts for model transformation. Additionally, it provides a comparative analysis of these methods across various aspects. 1. AI-Assisted Model Transformation Description: This approach utilizes large language models (such as ChatGPT or T5) to transform rule or model files. These models are adept at handling structured or semi-structured data and are particularly suited for semantic mapping between different rule formats. The migration process involves collecting the original rule data (e.g., ILOG rules), extracting common patterns or templates, analyzing the target rule format requirements, and forming a comprehensive transformation map to allow the model to understand and perform the conversion. Testing with Prompt Engineering: This method designs prompts tailored to large models. For instance, one can craft specific prompts to guide the model in understanding the structure and semantic mapping of rules. Testing with Fine-tuning: In this approach, the existing dataset is used to fine-tune the large language model. The model learns through parameter adjustments, thereby achieving improved performance for the task. Advantages and Disadvantages: Advantages: Rapid processing of large numbers of rules, especially when rules follow fixed patterns. No in-depth understanding of rule syntax is required, as long as reasonable prompts are designed. Disadvantages: The model’s ability to handle complex logic is limited, and manual corrections may be necessary. When dealing with high-complexity rules, the model may make errors. Additionally, the fine-tuning effect is difficult to control. 2. Using ANTLR for Model Transformation Description: ANTLR (Another Tool for Language Recognition) is a powerful syntax parsing tool that allows custom parsers to transform rule languages from one format to another. This approach is suitable for scenarios that require high precision and control over the parsing and transformation process. Implementation Steps: Define Rule Syntax: Design syntax files for both the original and target rule languages. Generate Parser: Use the ANTLR tool to generate parsers (supporting languages like Java, Python, etc.). Write Conversion Logic: Implement the rule conversion logic using the generated parsers. Advantages and Disadvantages: Advantages: High flexibility, allowing for fully customized parsing and transformation rules, making it suitable for complex scenarios. High precision in parsing and conversion, ideal for migrating complex logic. Strong scalability, with the ability to continuously adjust the parser to accommodate new rules. Disadvantages: A long development cycle, requiring deep understanding of both the original and target rule syntax. High learning curve for teams unfamiliar with the grammar. Initial development costs are significant, and maintaining the custom parsers requires expertise. 3. Manual and Python Script-Based Model Transformation Description: This approach involves manually analyzing the rule file structure and writing Python scripts to transform the rules from one format to another. It is most effective for scenarios where the number of rules is small, and the structure is relatively simple. Implementation Steps: Rule Analysis: Manually analyze the original rule file format and document the structural differences between the original and target rule formats. Write Script: Use Python (combined with regular expressions or basic parsing libraries) to write rule extraction and transformation scripts. Example: import re def convert_rule(input_rule): match = re.match(r'if (.+) then (.+);', input_rule) if match: condition, action = match.groups() return f'rule "{action.strip()}" when {condition} then {action};' Batch Processing: Feed the rule files into the script in bulk and output the converted target rule files. Manual Verification: Perform manual checks on the converted results and address any inconsistencies. Advantages and Disadvantages: Advantages: Simple implementation with low learning costs, suitable for small-scale projects. High flexibility, allowing rapid adjustments to the script based on rule variations. Low initial cost, requiring no additional tools or resources. Disadvantages: Limited ability to handle complex rule logic, with inefficient processing for intricate rules. Manual analysis and validation are time-consuming, limiting the scalability of this approach. Conversion accuracy is highly dependent on script quality, and high manual intervention may be required. 4. Comparative Analysis 4.1 Suitable Scenarios Comparison: Method Suitable Scenarios AI-Assisted Migration Scenarios where rule patterns are clear and semantic consistency is high. Suitable for natural language rule conversions or situations where existing large models can be leveraged. ANTLR Migration High-complexity rules with strict syntax and precision requirements. Ideal for large-scale or long-term rule migration needs. Manual and Python Script Migration Small-scale projects with a low number of rules and simple rule structures. Best suited for one-time migrations or projects lacking specialized tools. 5. Solution Selection AI-Assisted Migration: Based on testing, prompt-based templates provide a high degree of understanding and accuracy for model conversions. However, ChatGPT, due to security concerns, requires API access or deployment via Microsoft’s independent solution. Open-source large models, when deployed locally, do not provide satisfactory results for prompt engineering and fine-tuning. We used T5-large (770M, 770 million parameters) as the model size; unfortunately, we cannot test models with 3 billion (3B) or 11 billion (11B) parameters locally due to hardware limitations. The GPT-4 model, with parameter counts between 200 billion to 300 billion, offers potential for more powerful results. ANTLR Parser: The ANTLR-based solution is promising, but requires a thorough understanding of ILOG’s syntax and lexicon to create a versatile tool capable of handling various scenarios. Developing a generalized tool may prove challenging. Manual and Python Script Migration: This approach can be swiftly implemented based on the current syntax and lexicon. It is effective for targeted rule mapping, using hardcoded approaches for continuous processing. Uncommon cases can be added incrementally, with verification and supplementation through manual intervention and scripting. Conclusion The choice of rule migration approach largely depends on the specific needs of the migration project, including the complexity of the rules, the scale of the migration, and the available resources. AI-assisted methods offer speed but may require significant refinement, ANTLR offers high precision but at the cost of complexity and longer development cycles, while manual and Python script-based approaches are ideal for smaller, simpler projects but may lack scalability. Each method offers distinct advantages and trade-offs, and careful consideration of the specific use case is required to select the most appropriate solution for efficient and accurate rule migration.]]></summary></entry><entry><title type="html">Stability Monitor Alertmanager</title><link href="http://localhost:4000/blog/stability-monitor-alertmanager/" rel="alternate" type="text/html" title="Stability Monitor Alertmanager" /><published>2025-01-14T00:00:00+08:00</published><updated>2025-01-14T00:00:00+08:00</updated><id>http://localhost:4000/blog/stability-monitor-alertmanager</id><content type="html" xml:base="http://localhost:4000/blog/stability-monitor-alertmanager/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>Alertmanager is a tool designed to handle alerts sent by Prometheus, providing features such as deduplication, grouping, and routing of alerts to various receivers like email, Slack, or custom webhooks. This article demonstrates the process of installing and configuring Alertmanager, ensuring it operates as a service, and integrating it with Prometheus to handle alert notifications effectively.</p>

<h3 id="step-1-download-alertmanager">Step 1: Download Alertmanager</h3>

<p>The first step is to download the Alertmanager binary from the official GitHub repository. You can download the latest release from <a href="https://github.com/prometheus/alertmanager/releases">Prometheus’s Alertmanager GitHub page</a>.</p>

<p>For example, to download version 0.24.0, run the following command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget https://github.com/prometheus/alertmanager/releases/download/v0.24.0/alertmanager-0.24.0-linux-amd64.tar.gz
</code></pre></div></div>

<h3 id="step-2-extract-and-move-alertmanager-to-a-suitable-location">Step 2: Extract and Move Alertmanager to a Suitable Location</h3>

<p>Once the file is downloaded, extract it and move the binary to a directory included in the system’s <code class="language-plaintext highlighter-rouge">$PATH</code> (e.g., <code class="language-plaintext highlighter-rouge">/usr/local/bin/</code>):</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">tar</span> <span class="nt">-xvzf</span> alertmanager-0.24.0-linux-amd64.tar.gz
<span class="nb">mv </span>alertmanager-0.24.0-linux-amd64/alertmanager /usr/local/bin/
</code></pre></div></div>

<p>Next, grant execute permissions to the Alertmanager binary:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">chmod</span> +x /usr/local/bin/alertmanager
</code></pre></div></div>

<h3 id="step-3-create-alertmanager-configuration-file">Step 3: Create Alertmanager Configuration File</h3>

<p>Alertmanager requires a configuration file (<code class="language-plaintext highlighter-rouge">alertmanager.yml</code>) to define how it handles incoming alerts and how they should be routed to notification receivers.</p>

<p>Create the configuration directory and the file:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mkdir</span> <span class="nt">-p</span> /etc/alertmanager
vim /etc/alertmanager/alertmanager.yml
</code></pre></div></div>

<p>Here is a sample configuration file to get started:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">global</span><span class="pi">:</span>
  <span class="na">resolve_timeout</span><span class="pi">:</span> <span class="s">5m</span>

<span class="na">route</span><span class="pi">:</span>
  <span class="na">group_by</span><span class="pi">:</span> <span class="pi">[</span><span class="s1">'</span><span class="s">alertname'</span><span class="pi">]</span>
  <span class="na">receiver</span><span class="pi">:</span> <span class="s1">'</span><span class="s">email-config'</span>

<span class="na">receivers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s1">'</span><span class="s">email-config'</span>
    <span class="na">email_configs</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">to</span><span class="pi">:</span> <span class="s1">'</span><span class="s">your-email@example.com'</span>
        <span class="na">from</span><span class="pi">:</span> <span class="s1">'</span><span class="s">alertmanager@example.com'</span>
        <span class="na">smarthost</span><span class="pi">:</span> <span class="s1">'</span><span class="s">smtp.example.com:587'</span>
        <span class="na">auth_username</span><span class="pi">:</span> <span class="s1">'</span><span class="s">your-username'</span>
        <span class="na">auth_password</span><span class="pi">:</span> <span class="s1">'</span><span class="s">your-password'</span>
        <span class="na">require_tls</span><span class="pi">:</span> <span class="no">true</span>

<span class="na">group_interval</span><span class="pi">:</span> <span class="s">5m</span>
</code></pre></div></div>

<p>This configuration sets up a route that groups alerts by their <code class="language-plaintext highlighter-rouge">alertname</code> and sends notifications via email. You can customize this for other notification types (e.g., Slack, PagerDuty, etc.).</p>

<h3 id="step-4-create-systemd-service-file">Step 4: Create Systemd Service File</h3>

<p>To ensure that Alertmanager starts automatically at system boot and runs as a background service, create a <code class="language-plaintext highlighter-rouge">systemd</code> service file:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vim /etc/systemd/system/alertmanager.service
</code></pre></div></div>

<p>Add the following content to the file:</p>

<div class="language-ini highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">[Unit]</span>
<span class="py">Description</span><span class="p">=</span><span class="s">Alertmanager - A tool to handle alerts sent by Prometheus</span>
<span class="py">Documentation</span><span class="p">=</span><span class="s">https://prometheus.io/docs/alerting/latest/alertmanager/</span>
<span class="py">After</span><span class="p">=</span><span class="s">network.target</span>

<span class="nn">[Service]</span>
<span class="py">ExecStart</span><span class="p">=</span><span class="s">/usr/local/bin/alertmanager --config.file=/etc/alertmanager/alertmanager.yml</span>
<span class="py">Restart</span><span class="p">=</span><span class="s">on-failure</span>
<span class="py">User</span><span class="p">=</span><span class="s">delian</span>
<span class="py">Group</span><span class="p">=</span><span class="s">delian</span>

<span class="nn">[Install]</span>
<span class="py">WantedBy</span><span class="p">=</span><span class="s">multi-user.target</span>
</code></pre></div></div>

<h3 id="step-5-start-and-enable-alertmanager-service">Step 5: Start and Enable Alertmanager Service</h3>

<p>Once the service file is created, reload the <code class="language-plaintext highlighter-rouge">systemd</code> manager to register the new service and start it. Additionally, enable it to start automatically at boot time:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>systemctl daemon-reload
systemctl start alertmanager
systemctl <span class="nb">enable </span>alertmanager
systemctl status alertmanager
</code></pre></div></div>

<p>Alertmanager will now run in the background and handle alerts according to the configuration file.</p>

<h3 id="step-6-configuring-alerting-rules-in-prometheus">Step 6: Configuring Alerting Rules in Prometheus</h3>

<p>To trigger alerts, Prometheus requires alerting rules. These rules define the conditions under which an alert should be fired. Alerting rules can be defined in the <code class="language-plaintext highlighter-rouge">prometheus.yml</code> configuration file or in a separate rules file.</p>

<h4 id="step-61-define-alerting-rules-in-prometheus">Step 6.1: Define Alerting Rules in Prometheus</h4>

<p>Create a separate file for alerting rules (e.g., <code class="language-plaintext highlighter-rouge">alert.rules.yml</code>), or define the rules directly in <code class="language-plaintext highlighter-rouge">prometheus.yml</code>. Below is an example of an alert rules file:</p>

<p>Create a rules file:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vim /etc/prometheus/alert.rules.yml
</code></pre></div></div>

<p>Add the following alerting rules:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">groups</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">example-alerts</span>
  <span class="na">rules</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">alert</span><span class="pi">:</span> <span class="s">HighCPUUsage</span>
    <span class="na">expr</span><span class="pi">:</span> <span class="s">avg(rate(cpu_usage_seconds_total{mode="user"}[5m])) by (instance) &gt; </span><span class="m">0.9</span>
    <span class="na">for</span><span class="pi">:</span> <span class="s">5m</span>
    <span class="na">labels</span><span class="pi">:</span>
      <span class="na">severity</span><span class="pi">:</span> <span class="s">critical</span>
    <span class="na">annotations</span><span class="pi">:</span>
      <span class="na">summary</span><span class="pi">:</span> <span class="s2">"</span><span class="s">High</span><span class="nv"> </span><span class="s">CPU</span><span class="nv"> </span><span class="s">usage</span><span class="nv"> </span><span class="s">on</span><span class="nv"> </span><span class="s">"</span>
      <span class="na">description</span><span class="pi">:</span> <span class="s2">"</span><span class="s">CPU</span><span class="nv"> </span><span class="s">usage</span><span class="nv"> </span><span class="s">is</span><span class="nv"> </span><span class="s">above</span><span class="nv"> </span><span class="s">90%</span><span class="nv"> </span><span class="s">for</span><span class="nv"> </span><span class="s">the</span><span class="nv"> </span><span class="s">last</span><span class="nv"> </span><span class="s">5</span><span class="nv"> </span><span class="s">minutes</span><span class="nv"> </span><span class="s">on</span><span class="nv"> </span><span class="s">."</span>

  <span class="pi">-</span> <span class="na">alert</span><span class="pi">:</span> <span class="s">DiskSpaceLow</span>
    <span class="na">expr</span><span class="pi">:</span> <span class="s">(node_filesystem_free_bytes / node_filesystem_size_bytes) * 100 &lt; </span><span class="m">10</span>
    <span class="na">for</span><span class="pi">:</span> <span class="s">10m</span>
    <span class="na">labels</span><span class="pi">:</span>
      <span class="na">severity</span><span class="pi">:</span> <span class="s">warning</span>
    <span class="na">annotations</span><span class="pi">:</span>
      <span class="na">summary</span><span class="pi">:</span> <span class="s2">"</span><span class="s">Low</span><span class="nv"> </span><span class="s">disk</span><span class="nv"> </span><span class="s">space</span><span class="nv"> </span><span class="s">on</span><span class="nv"> </span><span class="s">"</span>
      <span class="na">description</span><span class="pi">:</span> <span class="s2">"</span><span class="s">Disk</span><span class="nv"> </span><span class="s">space</span><span class="nv"> </span><span class="s">on</span><span class="nv">  </span><span class="s">is</span><span class="nv"> </span><span class="s">below</span><span class="nv"> </span><span class="s">10%."</span>
</code></pre></div></div>

<p>Update the <code class="language-plaintext highlighter-rouge">prometheus.yml</code> file to include the alert rules:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">rule_files</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">/etc/prometheus/alert.rules.yml</span>
</code></pre></div></div>

<h4 id="step-62-ensure-prometheus-sends-alerts-to-alertmanager">Step 6.2: Ensure Prometheus Sends Alerts to Alertmanager</h4>

<p>Make sure Prometheus is configured to send alerts to Alertmanager. Modify the <code class="language-plaintext highlighter-rouge">alerting</code> section in the <code class="language-plaintext highlighter-rouge">prometheus.yml</code> file:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">alerting</span><span class="pi">:</span>
  <span class="na">alertmanagers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">static_configs</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">targets</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="s1">'</span><span class="s">localhost:9093'</span>
</code></pre></div></div>

<p>Reload Prometheus to apply the changes:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>systemctl reload prometheus
</code></pre></div></div>

<h3 id="step-7-testing-and-verifying-alerts">Step 7: Testing and Verifying Alerts</h3>

<p>After setting up the alerting rules, it’s important to test that the alerts are correctly triggered and routed.</p>

<ol>
  <li>
    <p><strong>Verify Alertmanager Receives Alerts</strong>: Visit the Alertmanager web interface (<code class="language-plaintext highlighter-rouge">http://&lt;alertmanager-ip&gt;:9093</code>) to check if alerts are being received and processed.</p>
  </li>
  <li>
    <p><strong>Test Alert Triggers</strong>: You can artificially trigger alert conditions (e.g., simulate high CPU usage) to test the rule.</p>
  </li>
  <li>
    <p><strong>Check Notification Receivers</strong>: Ensure that the notifications are sent to the configured receivers (e.g., email, Slack).</p>
  </li>
</ol>

<h2 id="conclusion">Conclusion</h2>

<p>In this article, we have walked through the installation and configuration of Alertmanager, as well as the setup of alerting rules in Prometheus. By following the outlined steps, you can ensure that your monitoring system is capable of detecting and notifying you about critical infrastructure issues, helping you to maintain operational reliability.</p>

<p>By configuring both Prometheus and Alertmanager, you set up a robust alerting mechanism, which will notify you in a timely manner about critical problems within your infrastructure.</p>]]></content><author><name>Madden Zhang</name></author><category term="Blog" /><category term="monitor" /><category term="alertmanager" /><summary type="html"><![CDATA[Introduction Alertmanager is a tool designed to handle alerts sent by Prometheus, providing features such as deduplication, grouping, and routing of alerts to various receivers like email, Slack, or custom webhooks. This article demonstrates the process of installing and configuring Alertmanager, ensuring it operates as a service, and integrating it with Prometheus to handle alert notifications effectively. Step 1: Download Alertmanager The first step is to download the Alertmanager binary from the official GitHub repository. You can download the latest release from Prometheus’s Alertmanager GitHub page. For example, to download version 0.24.0, run the following command: wget https://github.com/prometheus/alertmanager/releases/download/v0.24.0/alertmanager-0.24.0-linux-amd64.tar.gz Step 2: Extract and Move Alertmanager to a Suitable Location Once the file is downloaded, extract it and move the binary to a directory included in the system’s $PATH (e.g., /usr/local/bin/): tar -xvzf alertmanager-0.24.0-linux-amd64.tar.gz mv alertmanager-0.24.0-linux-amd64/alertmanager /usr/local/bin/ Next, grant execute permissions to the Alertmanager binary: chmod +x /usr/local/bin/alertmanager Step 3: Create Alertmanager Configuration File Alertmanager requires a configuration file (alertmanager.yml) to define how it handles incoming alerts and how they should be routed to notification receivers. Create the configuration directory and the file: mkdir -p /etc/alertmanager vim /etc/alertmanager/alertmanager.yml Here is a sample configuration file to get started: global: resolve_timeout: 5m route: group_by: ['alertname'] receiver: 'email-config' receivers: - name: 'email-config' email_configs: - to: 'your-email@example.com' from: 'alertmanager@example.com' smarthost: 'smtp.example.com:587' auth_username: 'your-username' auth_password: 'your-password' require_tls: true group_interval: 5m This configuration sets up a route that groups alerts by their alertname and sends notifications via email. You can customize this for other notification types (e.g., Slack, PagerDuty, etc.). Step 4: Create Systemd Service File To ensure that Alertmanager starts automatically at system boot and runs as a background service, create a systemd service file: vim /etc/systemd/system/alertmanager.service Add the following content to the file: [Unit] Description=Alertmanager - A tool to handle alerts sent by Prometheus Documentation=https://prometheus.io/docs/alerting/latest/alertmanager/ After=network.target [Service] ExecStart=/usr/local/bin/alertmanager --config.file=/etc/alertmanager/alertmanager.yml Restart=on-failure User=delian Group=delian [Install] WantedBy=multi-user.target Step 5: Start and Enable Alertmanager Service Once the service file is created, reload the systemd manager to register the new service and start it. Additionally, enable it to start automatically at boot time: systemctl daemon-reload systemctl start alertmanager systemctl enable alertmanager systemctl status alertmanager Alertmanager will now run in the background and handle alerts according to the configuration file. Step 6: Configuring Alerting Rules in Prometheus To trigger alerts, Prometheus requires alerting rules. These rules define the conditions under which an alert should be fired. Alerting rules can be defined in the prometheus.yml configuration file or in a separate rules file. Step 6.1: Define Alerting Rules in Prometheus Create a separate file for alerting rules (e.g., alert.rules.yml), or define the rules directly in prometheus.yml. Below is an example of an alert rules file: Create a rules file: vim /etc/prometheus/alert.rules.yml Add the following alerting rules: groups: - name: example-alerts rules: - alert: HighCPUUsage expr: avg(rate(cpu_usage_seconds_total{mode="user"}[5m])) by (instance) &gt; 0.9 for: 5m labels: severity: critical annotations: summary: "High CPU usage on " description: "CPU usage is above 90% for the last 5 minutes on ." - alert: DiskSpaceLow expr: (node_filesystem_free_bytes / node_filesystem_size_bytes) * 100 &lt; 10 for: 10m labels: severity: warning annotations: summary: "Low disk space on " description: "Disk space on is below 10%." Update the prometheus.yml file to include the alert rules: rule_files: - /etc/prometheus/alert.rules.yml Step 6.2: Ensure Prometheus Sends Alerts to Alertmanager Make sure Prometheus is configured to send alerts to Alertmanager. Modify the alerting section in the prometheus.yml file: alerting: alertmanagers: - static_configs: - targets: - 'localhost:9093' Reload Prometheus to apply the changes: systemctl reload prometheus Step 7: Testing and Verifying Alerts After setting up the alerting rules, it’s important to test that the alerts are correctly triggered and routed. Verify Alertmanager Receives Alerts: Visit the Alertmanager web interface (http://&lt;alertmanager-ip&gt;:9093) to check if alerts are being received and processed. Test Alert Triggers: You can artificially trigger alert conditions (e.g., simulate high CPU usage) to test the rule. Check Notification Receivers: Ensure that the notifications are sent to the configured receivers (e.g., email, Slack). Conclusion In this article, we have walked through the installation and configuration of Alertmanager, as well as the setup of alerting rules in Prometheus. By following the outlined steps, you can ensure that your monitoring system is capable of detecting and notifying you about critical infrastructure issues, helping you to maintain operational reliability. By configuring both Prometheus and Alertmanager, you set up a robust alerting mechanism, which will notify you in a timely manner about critical problems within your infrastructure.]]></summary></entry><entry><title type="html">Stability Monitor Promtail</title><link href="http://localhost:4000/blog/stability-monitor-promtail/" rel="alternate" type="text/html" title="Stability Monitor Promtail" /><published>2025-01-14T00:00:00+08:00</published><updated>2025-01-14T00:00:00+08:00</updated><id>http://localhost:4000/blog/stability-monitor-promtail</id><content type="html" xml:base="http://localhost:4000/blog/stability-monitor-promtail/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>Promtail is an agent that collects logs from various sources and sends them to Loki for storage and querying. In this section, we will walk through the installation process of Promtail, which includes downloading the necessary files, setting up configuration, and creating a service to ensure Promtail runs continuously as a background service.</p>

<h3 id="step-1-download-promtail">Step 1: Download Promtail</h3>

<p>The first step is to download the Promtail binary from the official GitHub repository. You can find the latest release <a href="https://github.com/grafana/loki/releases">here</a>.</p>

<p>For example, to download version 3.1.2, you can execute the following command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget https://github.com/grafana/loki/releases/download/v2.8.0/promtail-linux-amd64.zip
</code></pre></div></div>

<h3 id="step-2-extract-and-move-promtail-to-a-suitable-location">Step 2: Extract and Move Promtail to a Suitable Location</h3>

<p>Once the file is downloaded, extract it and move the binary to a directory included in the system’s <code class="language-plaintext highlighter-rouge">$PATH</code>, such as <code class="language-plaintext highlighter-rouge">/usr/local/bin/</code>.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>unzip promtail-linux-amd64.zip
<span class="nb">mv </span>promtail-linux-amd64 /usr/local/bin/promtail
</code></pre></div></div>

<p>Next, grant execute permissions to the Promtail binary to allow it to run:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">chmod</span> +x /usr/local/bin/promtail
</code></pre></div></div>

<h3 id="step-3-create-promtail-configuration-file">Step 3: Create Promtail Configuration File</h3>

<p>Promtail requires a configuration file that defines how logs will be collected, where they will be sent, and other parameters. Create the configuration directory and the file as follows:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mkdir</span> <span class="nt">-p</span> /etc/promtail
vim /etc/promtail/promtail-config.yaml
</code></pre></div></div>

<p>Below is a sample configuration file you can use, which specifies the Promtail server settings, position tracking, Loki client configuration, and the log scraping setup.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">server</span><span class="pi">:</span>
  <span class="na">http_listen_port</span><span class="pi">:</span> <span class="m">9080</span>
  <span class="na">grpc_listen_port</span><span class="pi">:</span> <span class="m">0</span>

<span class="na">positions</span><span class="pi">:</span>
  <span class="na">filename</span><span class="pi">:</span> <span class="s">/tmp/positions.yaml</span>

<span class="na">clients</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">url</span><span class="pi">:</span> <span class="s">http://192.168.18.58:3100/loki/api/v1/push</span>
    <span class="na">batchsize</span><span class="pi">:</span> <span class="m">1048576</span>
    <span class="na">batchwait</span><span class="pi">:</span> <span class="s">2s</span>

<span class="na">scrape_configs</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">job_name</span><span class="pi">:</span> <span class="s">your-application-name</span>
    <span class="na">static_configs</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">targets</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="s">localhost</span>
        <span class="na">labels</span><span class="pi">:</span>
          <span class="na">job</span><span class="pi">:</span> <span class="s">your-application-name</span>
          <span class="na">__path__</span><span class="pi">:</span> <span class="s">/var/log/your-application-name/*log</span>
</code></pre></div></div>

<h3 id="step-4-create-systemd-service-file">Step 4: Create Systemd Service File</h3>

<p>In order to ensure that Promtail starts automatically during system boot and runs as a background service, we need to create a systemd service file.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vim /etc/systemd/system/promtail.service
</code></pre></div></div>

<p>Add the following content to the file:</p>

<div class="language-ini highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">[Unit]</span>
<span class="py">Description</span><span class="p">=</span><span class="s">Promtail - A log collection agent for Loki</span>
<span class="py">Documentation</span><span class="p">=</span><span class="s">https://grafana.com/docs/loki/latest/clients/promtail/</span>
<span class="py">After</span><span class="p">=</span><span class="s">network.target</span>

<span class="nn">[Service]</span>
<span class="py">ExecStart</span><span class="p">=</span><span class="s">/usr/local/bin/promtail -config.file=/etc/promtail/promtail-config.yaml</span>
<span class="py">Restart</span><span class="p">=</span><span class="s">on-failure</span>
<span class="py">User</span><span class="p">=</span><span class="s">delian</span>
<span class="py">Group</span><span class="p">=</span><span class="s">delian</span>

<span class="nn">[Install]</span>
<span class="py">WantedBy</span><span class="p">=</span><span class="s">multi-user.target</span>
</code></pre></div></div>

<h3 id="step-5-start-and-enable-promtail-service">Step 5: Start and Enable Promtail Service</h3>

<p>After creating the systemd service file, reload the systemd manager to register the new service and start it. Additionally, enable it to start automatically at boot time.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>systemctl daemon-reload
systemctl start promtail
systemctl <span class="nb">enable </span>promtail
systemctl status promtail
</code></pre></div></div>

<p>The service will now run in the background, collecting and forwarding logs to Loki according to the specified configuration.</p>

<hr />

<h2 id="conclusion">Conclusion</h2>

<p>In this article, we have outlined the necessary steps to install and configure Promtail. By following these procedures, you will have a fully functional Promtail agent collecting logs and sending them to Loki, ready for real-time log monitoring.</p>]]></content><author><name>Madden Zhang</name></author><category term="Blog" /><category term="monitor" /><category term="stability" /><summary type="html"><![CDATA[Introduction Promtail is an agent that collects logs from various sources and sends them to Loki for storage and querying. In this section, we will walk through the installation process of Promtail, which includes downloading the necessary files, setting up configuration, and creating a service to ensure Promtail runs continuously as a background service. Step 1: Download Promtail The first step is to download the Promtail binary from the official GitHub repository. You can find the latest release here. For example, to download version 3.1.2, you can execute the following command: wget https://github.com/grafana/loki/releases/download/v2.8.0/promtail-linux-amd64.zip Step 2: Extract and Move Promtail to a Suitable Location Once the file is downloaded, extract it and move the binary to a directory included in the system’s $PATH, such as /usr/local/bin/. unzip promtail-linux-amd64.zip mv promtail-linux-amd64 /usr/local/bin/promtail Next, grant execute permissions to the Promtail binary to allow it to run: chmod +x /usr/local/bin/promtail Step 3: Create Promtail Configuration File Promtail requires a configuration file that defines how logs will be collected, where they will be sent, and other parameters. Create the configuration directory and the file as follows: mkdir -p /etc/promtail vim /etc/promtail/promtail-config.yaml Below is a sample configuration file you can use, which specifies the Promtail server settings, position tracking, Loki client configuration, and the log scraping setup. server: http_listen_port: 9080 grpc_listen_port: 0 positions: filename: /tmp/positions.yaml clients: - url: http://192.168.18.58:3100/loki/api/v1/push batchsize: 1048576 batchwait: 2s scrape_configs: - job_name: your-application-name static_configs: - targets: - localhost labels: job: your-application-name __path__: /var/log/your-application-name/*log Step 4: Create Systemd Service File In order to ensure that Promtail starts automatically during system boot and runs as a background service, we need to create a systemd service file. vim /etc/systemd/system/promtail.service Add the following content to the file: [Unit] Description=Promtail - A log collection agent for Loki Documentation=https://grafana.com/docs/loki/latest/clients/promtail/ After=network.target [Service] ExecStart=/usr/local/bin/promtail -config.file=/etc/promtail/promtail-config.yaml Restart=on-failure User=delian Group=delian [Install] WantedBy=multi-user.target Step 5: Start and Enable Promtail Service After creating the systemd service file, reload the systemd manager to register the new service and start it. Additionally, enable it to start automatically at boot time. systemctl daemon-reload systemctl start promtail systemctl enable promtail systemctl status promtail The service will now run in the background, collecting and forwarding logs to Loki according to the specified configuration. Conclusion In this article, we have outlined the necessary steps to install and configure Promtail. By following these procedures, you will have a fully functional Promtail agent collecting logs and sending them to Loki, ready for real-time log monitoring.]]></summary></entry><entry><title type="html">Stability Monitor Prometheus</title><link href="http://localhost:4000/blog/stability-monitor-prometheus/" rel="alternate" type="text/html" title="Stability Monitor Prometheus" /><published>2025-01-11T00:00:00+08:00</published><updated>2025-01-11T00:00:00+08:00</updated><id>http://localhost:4000/blog/stability-monitor-prometheus</id><content type="html" xml:base="http://localhost:4000/blog/stability-monitor-prometheus/"><![CDATA[<h2 id="background">Background</h2>
<p>A significant part of system stability is supported by monitoring. Large companies usually have well-established monitoring and operations teams to build the monitoring infrastructure. From a layered perspective, monitoring generally includes the following aspects:</p>
<table>
  <thead>
    <tr>
      <th>Monitoring Dimension</th>
      <th>Middleware Selection</th>
      <th>Reason for Selection</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Metric Monitoring</td>
      <td>Prometheus + Grafana</td>
      <td>Supports multiple Exporters, rich ecosystem, easy to configure alerts and visualizations</td>
    </tr>
    <tr>
      <td>Log Monitoring</td>
      <td>Loki + Promtail/Fluent Bit</td>
      <td>Lightweight log aggregation solution, seamlessly integrates with Grafana</td>
    </tr>
    <tr>
      <td>Distributed Tracing</td>
      <td>OpenTelemetry + Jaeger</td>
      <td>Open standard for distributed tracing, supports multiple languages</td>
    </tr>
    <tr>
      <td>Database Monitoring</td>
      <td>Exporter (e.g., MySQL Exporter)</td>
      <td>Prometheus maintained by official or community, supports mainstream databases</td>
    </tr>
    <tr>
      <td>Network Monitoring</td>
      <td>Blackbox Exporter</td>
      <td>Supports multi-protocol health checks like HTTP, TCP</td>
    </tr>
    <tr>
      <td>Alerting and Notification</td>
      <td>Alertmanager</td>
      <td>Supports multi-channel notifications (email, Slack, Webhook, SMS, etc.)</td>
    </tr>
  </tbody>
</table>

<h2 id="best-practices-for-selection">Best Practices for Selection</h2>
<p>Small and medium-sized companies can quickly build a monitoring system that suits their business characteristics. Prometheus has already become the standard for real-time monitoring. We can quickly set up our own monitoring system based on Prometheus:</p>
<table>
  <thead>
    <tr>
      <th>Monitoring Dimension</th>
      <th>Middleware Selection</th>
      <th>Reason for Selection</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Metric Monitoring</td>
      <td>Prometheus + Grafana</td>
      <td>Supports multiple Exporters, rich ecosystem, easy to configure alerts and visualizations</td>
    </tr>
    <tr>
      <td>Log Monitoring</td>
      <td>Loki + Promtail/Fluent Bit</td>
      <td>Lightweight log aggregation solution, seamlessly integrates with Grafana</td>
    </tr>
    <tr>
      <td>Distributed Tracing</td>
      <td>OpenTelemetry + Jaeger</td>
      <td>Open standard for distributed tracing, supports multiple languages</td>
    </tr>
    <tr>
      <td>Database Monitoring</td>
      <td>Exporter (e.g., MySQL Exporter, Redis Exporter)</td>
      <td>Prometheus maintained by official or community, supports mainstream databases</td>
    </tr>
    <tr>
      <td>Network Monitoring</td>
      <td>Blackbox Exporter</td>
      <td>Supports multi-protocol health checks like HTTP, TCP</td>
    </tr>
    <tr>
      <td>Alerting and Notification</td>
      <td>Alertmanager</td>
      <td>Supports multi-channel notifications (email, Slack, Webhook, SMS, etc.)</td>
    </tr>
  </tbody>
</table>

<h2 id="system-architecture-design">System Architecture Design</h2>
<div class="mermaid">
  graph TD;
    A[Prometheus] --&gt; B[Exporters]
    A --&gt; C[Blackbox Exporter]
    A --&gt; D[Alertmanager]
    B --&gt; E[Grafana]
    C --&gt; E
    D --&gt; E
    F[Loki] --&gt; G[Promtail/Fluent Bit]
    G --&gt; E
    H[OpenTelemetry] --&gt; I[Jaeger]
    I --&gt; E
</div>

<h2 id="defining-refined-monitoring-metrics">Defining Refined Monitoring Metrics</h2>
<h3 id="jvm-monitoring">JVM Monitoring</h3>

<p>JVM monitoring is used to track important JVM metrics, including GC (Garbage Collection) instant metrics, heap memory metrics, non-heap memory metrics, metaspace metrics, direct buffer metrics, JVM thread count, etc. This section introduces JVM monitoring and how to view JVM monitoring metrics.</p>

<p>JVM monitoring can track the following metrics:</p>

<ul>
  <li>GC (Garbage Collection) instant and cumulative details
    <ul>
      <li>FullGC count</li>
      <li>YoungGC count</li>
      <li>FullGC duration</li>
      <li>YoungGC duration</li>
    </ul>
  </li>
  <li>Heap Memory Details
    <ul>
      <li>Total heap memory</li>
      <li>Old generation heap memory size</li>
      <li>Young generation Survivor area size</li>
      <li>Young generation Eden area size</li>
    </ul>
  </li>
  <li>
    <p>Metaspace</p>

    <p>Metaspace size</p>
  </li>
  <li>Non-Heap Memory
    <ul>
      <li>Maximum non-heap memory size</li>
      <li>Used non-heap memory size</li>
    </ul>
  </li>
  <li>Direct Buffer
    <ul>
      <li>Total DirectBuffer size (bytes)</li>
      <li>Used DirectBuffer size (bytes)</li>
    </ul>
  </li>
  <li>JVM Thread Count
    <ul>
      <li>Total number of threads</li>
      <li>Number of deadlocked threads</li>
      <li>Number of newly created threads</li>
      <li>Number of blocked threads</li>
      <li>Number of runnable threads</li>
      <li>Number of terminated threads</li>
      <li>Number of threads in timed wait</li>
      <li>Number of threads in waiting state</li>
    </ul>
  </li>
</ul>

<div class="mermaid">
mindmap
  root((Java Process Memory Usage))
    JVM Memory
      Heap Memory
        Young Generation
        Old Generation
      Non-Heap Memory
        Metaspace
        Compressed Class Space
        Virtual Machine Thread Stack
        Native Thread Stack
        Code Cache
        Direct Buffers
    Non-JVM Memory
      Native Runtime Libraries
      JNI Native Code
</div>

<h3 id="host-monitoring">Host Monitoring</h3>

<p>Host monitoring tracks various metrics such as CPU, memory, disk, load, network traffic, and network packet metrics. This section introduces host monitoring and how to view host monitoring metrics.</p>

<p>Host monitoring can track the following metrics:</p>

<ul>
  <li>CPU
    <ul>
      <li>Total CPU usage</li>
      <li>System CPU usage</li>
      <li>User CPU usage</li>
      <li>CPU usage waiting for I/O completion</li>
    </ul>
  </li>
  <li>Physical Memory
    <ul>
      <li>Total system memory</li>
      <li>Free system memory</li>
      <li>Used system memory</li>
      <li>Memory in PageCache</li>
      <li>Memory in BufferCache</li>
    </ul>
  </li>
  <li>Disk
    <ul>
      <li>Total system disk size</li>
      <li>Free system disk size</li>
      <li>Used system disk size</li>
    </ul>
  </li>
  <li>
    <p>Load</p>

    <p>System load average</p>
  </li>
  <li>Network Traffic
    <ul>
      <li>Network received bytes</li>
      <li>Network sent bytes</li>
    </ul>
  </li>
  <li>Network Packets
    <ul>
      <li>Number of received packets per minute</li>
      <li>Number of sent packets per minute</li>
      <li>Number of network errors per minute</li>
      <li>Number of dropped packets per minute</li>
    </ul>
  </li>
</ul>

<h3 id="sql-call-analysis"><strong>SQL Call Analysis</strong></h3>

<p>View SQL call analysis to understand SQL call patterns in applications.</p>

<h3 id="error-code-monitoring">Error Code Monitoring</h3>

<p>For core business systems, such as payment systems, error code monitoring is essential.</p>

<p>Here’s how to install Prometheus step by step in English. If you use the docker you can use this to setup, this is the easy way. <a href="https://github.com/maddenmanel/springboot-prometheus-grafana">springboot-promethenus-grafana</a>.</p>

<h3 id="install-article-list">Install Article list</h3>

<ul>
  <li><a href="/tool/stability-monitor-loki/">Install Loki with Prometheus</a></li>
  <li><a href="/tool/stability-monitor-promtail/">Install Promtail with Prometheus</a></li>
</ul>

<script type="module">
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
  mermaid.initialize({ startOnLoad: true });
</script>]]></content><author><name>Madden Zhang</name></author><category term="Blog" /><category term="monitor" /><category term="stability" /><summary type="html"><![CDATA[Background A significant part of system stability is supported by monitoring. Large companies usually have well-established monitoring and operations teams to build the monitoring infrastructure. From a layered perspective, monitoring generally includes the following aspects: Monitoring Dimension Middleware Selection Reason for Selection Metric Monitoring Prometheus + Grafana Supports multiple Exporters, rich ecosystem, easy to configure alerts and visualizations Log Monitoring Loki + Promtail/Fluent Bit Lightweight log aggregation solution, seamlessly integrates with Grafana Distributed Tracing OpenTelemetry + Jaeger Open standard for distributed tracing, supports multiple languages Database Monitoring Exporter (e.g., MySQL Exporter) Prometheus maintained by official or community, supports mainstream databases Network Monitoring Blackbox Exporter Supports multi-protocol health checks like HTTP, TCP Alerting and Notification Alertmanager Supports multi-channel notifications (email, Slack, Webhook, SMS, etc.) Best Practices for Selection Small and medium-sized companies can quickly build a monitoring system that suits their business characteristics. Prometheus has already become the standard for real-time monitoring. We can quickly set up our own monitoring system based on Prometheus: Monitoring Dimension Middleware Selection Reason for Selection Metric Monitoring Prometheus + Grafana Supports multiple Exporters, rich ecosystem, easy to configure alerts and visualizations Log Monitoring Loki + Promtail/Fluent Bit Lightweight log aggregation solution, seamlessly integrates with Grafana Distributed Tracing OpenTelemetry + Jaeger Open standard for distributed tracing, supports multiple languages Database Monitoring Exporter (e.g., MySQL Exporter, Redis Exporter) Prometheus maintained by official or community, supports mainstream databases Network Monitoring Blackbox Exporter Supports multi-protocol health checks like HTTP, TCP Alerting and Notification Alertmanager Supports multi-channel notifications (email, Slack, Webhook, SMS, etc.) System Architecture Design graph TD; A[Prometheus] --&gt; B[Exporters] A --&gt; C[Blackbox Exporter] A --&gt; D[Alertmanager] B --&gt; E[Grafana] C --&gt; E D --&gt; E F[Loki] --&gt; G[Promtail/Fluent Bit] G --&gt; E H[OpenTelemetry] --&gt; I[Jaeger] I --&gt; E Defining Refined Monitoring Metrics JVM Monitoring JVM monitoring is used to track important JVM metrics, including GC (Garbage Collection) instant metrics, heap memory metrics, non-heap memory metrics, metaspace metrics, direct buffer metrics, JVM thread count, etc. This section introduces JVM monitoring and how to view JVM monitoring metrics. JVM monitoring can track the following metrics: GC (Garbage Collection) instant and cumulative details FullGC count YoungGC count FullGC duration YoungGC duration Heap Memory Details Total heap memory Old generation heap memory size Young generation Survivor area size Young generation Eden area size Metaspace Metaspace size Non-Heap Memory Maximum non-heap memory size Used non-heap memory size Direct Buffer Total DirectBuffer size (bytes) Used DirectBuffer size (bytes) JVM Thread Count Total number of threads Number of deadlocked threads Number of newly created threads Number of blocked threads Number of runnable threads Number of terminated threads Number of threads in timed wait Number of threads in waiting state mindmap root((Java Process Memory Usage)) JVM Memory Heap Memory Young Generation Old Generation Non-Heap Memory Metaspace Compressed Class Space Virtual Machine Thread Stack Native Thread Stack Code Cache Direct Buffers Non-JVM Memory Native Runtime Libraries JNI Native Code Host Monitoring Host monitoring tracks various metrics such as CPU, memory, disk, load, network traffic, and network packet metrics. This section introduces host monitoring and how to view host monitoring metrics. Host monitoring can track the following metrics: CPU Total CPU usage System CPU usage User CPU usage CPU usage waiting for I/O completion Physical Memory Total system memory Free system memory Used system memory Memory in PageCache Memory in BufferCache Disk Total system disk size Free system disk size Used system disk size Load System load average Network Traffic Network received bytes Network sent bytes Network Packets Number of received packets per minute Number of sent packets per minute Number of network errors per minute Number of dropped packets per minute SQL Call Analysis View SQL call analysis to understand SQL call patterns in applications. Error Code Monitoring For core business systems, such as payment systems, error code monitoring is essential. Here’s how to install Prometheus step by step in English. If you use the docker you can use this to setup, this is the easy way. springboot-promethenus-grafana. Install Article list Install Loki with Prometheus Install Promtail with Prometheus]]></summary></entry><entry><title type="html">Stability Monitor Loki</title><link href="http://localhost:4000/blog/stability-monitor-loki/" rel="alternate" type="text/html" title="Stability Monitor Loki" /><published>2025-01-11T00:00:00+08:00</published><updated>2025-01-11T00:00:00+08:00</updated><id>http://localhost:4000/blog/stability-monitor-loki</id><content type="html" xml:base="http://localhost:4000/blog/stability-monitor-loki/"><![CDATA[<h2 id="install-loki">Install Loki</h2>

<p>To install <strong>Loki</strong> version <strong>3.1.2</strong> on a Linux system, follow the steps below. Loki is an open-source log aggregation system developed by Grafana Labs, and you can install it using either pre-built binaries, Docker, or through package managers. We’ll use pre-built binaries for this guide.</p>

<h3 id="step-1-download-the-loki-binary">Step 1: Download the Loki Binary</h3>

<p>Go to the <a href="https://github.com/grafana/loki/releases">Loki GitHub releases page</a> and find version <strong>3.1.2</strong>. Or use <code class="language-plaintext highlighter-rouge">wget</code> to directly download the binary:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget https://github.com/grafana/loki/releases/download/v3.1.2/loki-linux-amd64.zip
</code></pre></div></div>

<p>Once downloaded, extract the <code class="language-plaintext highlighter-rouge">.zip</code> file:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>unzip loki-linux-amd64.zip
</code></pre></div></div>

<p>This will create a folder containing the Loki binary (<code class="language-plaintext highlighter-rouge">loki-linux-amd64</code>).</p>

<h3 id="step-2-move-loki-to-a-system-directory">Step 2: Move Loki to a System Directory</h3>

<p>Move the extracted binary to a directory in your PATH (e.g., <code class="language-plaintext highlighter-rouge">/usr/local/bin</code>):</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mv </span>loki-linux-amd64 /usr/local/bin/loki
</code></pre></div></div>

<p>Ensure the binary is executable:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">chmod</span> +x /usr/local/bin/loki
</code></pre></div></div>

<h3 id="step-3-create-a-configuration-file-optional">Step 3: Create a Configuration File (Optional)</h3>

<p>Create a directory for Loki configuration and add the <code class="language-plaintext highlighter-rouge">local-config.yaml</code> file:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mkdir</span> <span class="nt">-p</span> /etc/loki
vim /etc/loki/local-config.yaml
</code></pre></div></div>

<p>The content for <code class="language-plaintext highlighter-rouge">local-config.yaml</code>:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">auth_enabled</span><span class="pi">:</span> <span class="no">false</span>

<span class="na">server</span><span class="pi">:</span>
  <span class="na">http_listen_port</span><span class="pi">:</span> <span class="m">3100</span>
  <span class="na">grpc_listen_port</span><span class="pi">:</span> <span class="m">9095</span>

<span class="na">common</span><span class="pi">:</span>
  <span class="na">path_prefix</span><span class="pi">:</span> <span class="s">/loki/data</span>  

<span class="na">storage_config</span><span class="pi">:</span>
  <span class="na">boltdb_shipper</span><span class="pi">:</span>
    <span class="na">active_index_directory</span><span class="pi">:</span> <span class="s">/loki/index</span>
    <span class="na">cache_location</span><span class="pi">:</span> <span class="s">/loki/cache</span>
    <span class="na">resync_interval</span><span class="pi">:</span> <span class="s">10m</span>
  <span class="na">filesystem</span><span class="pi">:</span>
    <span class="na">directory</span><span class="pi">:</span> <span class="s">/loki/chunks</span>

<span class="na">limits_config</span><span class="pi">:</span>
  <span class="na">retention_period</span><span class="pi">:</span> <span class="s">90d</span>
  <span class="na">allow_structured_metadata</span><span class="pi">:</span> <span class="no">false</span> 

<span class="na">schema_config</span><span class="pi">:</span>
  <span class="na">configs</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">from</span><span class="pi">:</span> <span class="s">2020-10-21</span>
      <span class="na">store</span><span class="pi">:</span> <span class="s">boltdb-shipper</span>
      <span class="na">object_store</span><span class="pi">:</span> <span class="s">filesystem</span>
      <span class="na">schema</span><span class="pi">:</span> <span class="s">v13</span>
      <span class="na">index</span><span class="pi">:</span>
        <span class="na">prefix</span><span class="pi">:</span> <span class="s">index_</span>
        <span class="na">period</span><span class="pi">:</span> <span class="s">24h</span>  

<span class="na">compactor</span><span class="pi">:</span>
  <span class="na">working_directory</span><span class="pi">:</span> <span class="s">/loki/compactor</span> 

<span class="na">table_manager</span><span class="pi">:</span>
  <span class="na">retention_deletes_enabled</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">retention_period</span><span class="pi">:</span> <span class="s">90d</span>

<span class="na">ingester</span><span class="pi">:</span>
  <span class="na">lifecycler</span><span class="pi">:</span>
    <span class="na">ring</span><span class="pi">:</span>
      <span class="na">kvstore</span><span class="pi">:</span>
        <span class="na">store</span><span class="pi">:</span> <span class="s">memberlist</span>
      <span class="na">replication_factor</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">chunk_target_size</span><span class="pi">:</span> <span class="s">1048576</span>  
  <span class="na">max_chunk_age</span><span class="pi">:</span> <span class="s">1h</span> 

<span class="na">memberlist</span><span class="pi">:</span>
  <span class="na">join_members</span><span class="pi">:</span> <span class="pi">[]</span> 
</code></pre></div></div>

<h3 id="step-4-configure-log-file-storage">Step 4: Configure Log File Storage</h3>

<p>Define storage directories for Loki logs:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mkdir</span> <span class="nt">-p</span> /var/loki/chunks /var/loki/index /var/loki/cache /var/loki/compactor
<span class="nb">chown</span> <span class="nt">-R</span> loki:loki /var/loki
</code></pre></div></div>

<p>Create the Loki systemd service file:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vim /etc/systemd/system/loki.service
</code></pre></div></div>

<p>Add the following content:</p>

<div class="language-ini highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">[Unit]</span>
<span class="py">Description</span><span class="p">=</span><span class="s">Loki</span>
<span class="py">Documentation</span><span class="p">=</span><span class="s">https://grafana.com/docs/loki/latest/</span>
<span class="py">After</span><span class="p">=</span><span class="s">network.target</span>

<span class="nn">[Service]</span>
<span class="py">ExecStart</span><span class="p">=</span><span class="s">/usr/local/bin/loki -config.file=/etc/loki/local-config.yaml</span>
<span class="py">Restart</span><span class="p">=</span><span class="s">on-failure</span>
<span class="py">User</span><span class="p">=</span><span class="s">delian</span>
<span class="py">Group</span><span class="p">=</span><span class="s">delian</span>

<span class="nn">[Install]</span>
<span class="py">WantedBy</span><span class="p">=</span><span class="s">multi-user.target</span>
</code></pre></div></div>

<h3 id="step-5-start-loki-service">Step 5: Start Loki Service</h3>

<p>Reload systemd, enable and start the Loki service:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>systemctl daemon-reload
systemctl <span class="nb">enable </span>loki
systemctl start loki
systemctl status loki
</code></pre></div></div>

<p>To monitor logs:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>journalctl <span class="nt">-u</span> loki <span class="nt">-f</span>
</code></pre></div></div>

<p>Verify the Loki version:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>loki <span class="nt">--version</span>
</code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>

<p>By following the above steps, you should have Loki installed and running with the necessary configurations for log aggregation. You can now proceed to integrate Loki with other components like Promtail, Grafana, etc., for a complete logging solution.</p>]]></content><author><name>Madden Zhang</name></author><category term="Blog" /><category term="monitor" /><category term="stability" /><summary type="html"><![CDATA[Install Loki To install Loki version 3.1.2 on a Linux system, follow the steps below. Loki is an open-source log aggregation system developed by Grafana Labs, and you can install it using either pre-built binaries, Docker, or through package managers. We’ll use pre-built binaries for this guide. Step 1: Download the Loki Binary Go to the Loki GitHub releases page and find version 3.1.2. Or use wget to directly download the binary: wget https://github.com/grafana/loki/releases/download/v3.1.2/loki-linux-amd64.zip Once downloaded, extract the .zip file: unzip loki-linux-amd64.zip This will create a folder containing the Loki binary (loki-linux-amd64). Step 2: Move Loki to a System Directory Move the extracted binary to a directory in your PATH (e.g., /usr/local/bin): mv loki-linux-amd64 /usr/local/bin/loki Ensure the binary is executable: chmod +x /usr/local/bin/loki Step 3: Create a Configuration File (Optional) Create a directory for Loki configuration and add the local-config.yaml file: mkdir -p /etc/loki vim /etc/loki/local-config.yaml The content for local-config.yaml: auth_enabled: false server: http_listen_port: 3100 grpc_listen_port: 9095 common: path_prefix: /loki/data storage_config: boltdb_shipper: active_index_directory: /loki/index cache_location: /loki/cache resync_interval: 10m filesystem: directory: /loki/chunks limits_config: retention_period: 90d allow_structured_metadata: false schema_config: configs: - from: 2020-10-21 store: boltdb-shipper object_store: filesystem schema: v13 index: prefix: index_ period: 24h compactor: working_directory: /loki/compactor table_manager: retention_deletes_enabled: true retention_period: 90d ingester: lifecycler: ring: kvstore: store: memberlist replication_factor: 1 chunk_target_size: 1048576 max_chunk_age: 1h memberlist: join_members: [] Step 4: Configure Log File Storage Define storage directories for Loki logs: mkdir -p /var/loki/chunks /var/loki/index /var/loki/cache /var/loki/compactor chown -R loki:loki /var/loki Create the Loki systemd service file: vim /etc/systemd/system/loki.service Add the following content: [Unit] Description=Loki Documentation=https://grafana.com/docs/loki/latest/ After=network.target [Service] ExecStart=/usr/local/bin/loki -config.file=/etc/loki/local-config.yaml Restart=on-failure User=delian Group=delian [Install] WantedBy=multi-user.target Step 5: Start Loki Service Reload systemd, enable and start the Loki service: systemctl daemon-reload systemctl enable loki systemctl start loki systemctl status loki To monitor logs: journalctl -u loki -f Verify the Loki version: loki --version Conclusion By following the above steps, you should have Loki installed and running with the necessary configurations for log aggregation. You can now proceed to integrate Loki with other components like Promtail, Grafana, etc., for a complete logging solution.]]></summary></entry><entry><title type="html">Stability Monitor Jaeger</title><link href="http://localhost:4000/blog/stability-monitor-jaeger/" rel="alternate" type="text/html" title="Stability Monitor Jaeger" /><published>2025-01-11T00:00:00+08:00</published><updated>2025-01-11T00:00:00+08:00</updated><id>http://localhost:4000/blog/stability-monitor-jaeger</id><content type="html" xml:base="http://localhost:4000/blog/stability-monitor-jaeger/"><![CDATA[<h2 id="install-jaeger">Install Jaeger</h2>

<p>To install <strong>Jaeger</strong> version <strong>1.42.0</strong> on a Linux system, follow the steps below. Jaeger is an open-source distributed tracing system, and you can install it using pre-built binaries, Docker, or through package managers. This guide will cover the installation process using pre-built binaries.</p>

<h3 id="step-1-download-the-jaeger-binary">Step 1: Download the Jaeger Binary</h3>

<p>Go to the <a href="https://github.com/jaegertracing/jaeger/releases">Jaeger GitHub releases page</a> and find version <strong>1.42.0</strong>. Or use <code class="language-plaintext highlighter-rouge">wget</code> to directly download the binary:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget https://github.com/jaegertracing/jaeger/releases/download/v1.42.0/jaeger-linux-amd64.tar.gz
</code></pre></div></div>

<p>Once downloaded, extract the <code class="language-plaintext highlighter-rouge">.tar.gz</code> file:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">tar</span> <span class="nt">-xvzf</span> jaeger-linux-amd64.tar.gz
</code></pre></div></div>

<p>This will create a folder containing the Jaeger binaries (<code class="language-plaintext highlighter-rouge">jaeger-agent</code>, <code class="language-plaintext highlighter-rouge">jaeger-collector</code>, <code class="language-plaintext highlighter-rouge">jaeger-query</code>, etc.).</p>

<h3 id="step-2-move-jaeger-to-a-system-directory">Step 2: Move Jaeger to a System Directory</h3>

<p>Move the extracted binaries to a directory in your PATH (e.g., <code class="language-plaintext highlighter-rouge">/usr/local/bin</code>):</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mv </span>jaeger-linux-amd64 /usr/local/bin/jaeger
</code></pre></div></div>

<p>Ensure the binaries are executable:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">chmod</span> +x /usr/local/bin/jaeger
</code></pre></div></div>

<h3 id="step-3-create-a-configuration-file-optional">Step 3: Create a Configuration File (Optional)</h3>

<p>Create a directory for Jaeger configuration and add the <code class="language-plaintext highlighter-rouge">jaeger-config.yaml</code> file:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mkdir</span> <span class="nt">-p</span> /etc/jaeger
vim /etc/jaeger/jaeger-config.yaml
</code></pre></div></div>

<p>The content for <code class="language-plaintext highlighter-rouge">jaeger-config.yaml</code>:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">collector</span><span class="pi">:</span>
  <span class="na">http-port</span><span class="pi">:</span> <span class="m">5775</span>
  <span class="na">grpc-port</span><span class="pi">:</span> <span class="m">14250</span>

<span class="na">agent</span><span class="pi">:</span>
  <span class="na">http-port</span><span class="pi">:</span> <span class="m">5778</span>

<span class="na">query</span><span class="pi">:</span>
  <span class="na">http-port</span><span class="pi">:</span> <span class="m">16686</span>

<span class="na">storage</span><span class="pi">:</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">cassandra</span>
  <span class="na">cassandra</span><span class="pi">:</span>
    <span class="na">contact-points</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s2">"</span><span class="s">localhost:9042"</span>
    <span class="na">keyspace</span><span class="pi">:</span> <span class="s">jaeger</span>
    <span class="na">timeout</span><span class="pi">:</span> <span class="s">30s</span>
</code></pre></div></div>

<h3 id="step-4-configure-data-directories">Step 4: Configure Data Directories</h3>

<p>Define storage directories for Jaeger:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mkdir</span> <span class="nt">-p</span> /var/jaeger/cassandra /var/jaeger/storage
<span class="nb">chown</span> <span class="nt">-R</span> jaeger:jaeger /var/jaeger
</code></pre></div></div>

<p>Create the Jaeger systemd service file:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vim /etc/systemd/system/jaeger.service
</code></pre></div></div>

<p>Add the following content:</p>

<div class="language-ini highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">[Unit]</span>
<span class="py">Description</span><span class="p">=</span><span class="s">Jaeger</span>
<span class="py">Documentation</span><span class="p">=</span><span class="s">https://www.jaegertracing.io/docs/latest/</span>
<span class="py">After</span><span class="p">=</span><span class="s">network.target</span>

<span class="nn">[Service]</span>
<span class="py">ExecStart</span><span class="p">=</span><span class="s">/usr/local/bin/jaeger --config.file=/etc/jaeger/jaeger-config.yaml</span>
<span class="py">Restart</span><span class="p">=</span><span class="s">on-failure</span>
<span class="py">User</span><span class="p">=</span><span class="s">delian</span>
<span class="py">Group</span><span class="p">=</span><span class="s">delian</span>

<span class="nn">[Install]</span>
<span class="py">WantedBy</span><span class="p">=</span><span class="s">multi-user.target</span>
</code></pre></div></div>

<h3 id="step-5-start-jaeger-service">Step 5: Start Jaeger Service</h3>

<p>Reload systemd, enable and start the Jaeger service:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>systemctl daemon-reload
systemctl <span class="nb">enable </span>jaeger
systemctl start jaeger
systemctl status jaeger
</code></pre></div></div>

<p>To monitor logs:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>journalctl <span class="nt">-u</span> jaeger <span class="nt">-f</span>
</code></pre></div></div>

<p>Verify the Jaeger version:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jaeger <span class="nt">--version</span>
</code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>

<p>By following the above steps, you should have Jaeger installed and running with the necessary configurations for distributed tracing. You can now proceed to integrate Jaeger with other components like Prometheus, Grafana, etc., for a complete monitoring and tracing solution.</p>]]></content><author><name>Madden Zhang</name></author><category term="Blog" /><category term="monitor" /><category term="tracing" /><summary type="html"><![CDATA[Install Jaeger To install Jaeger version 1.42.0 on a Linux system, follow the steps below. Jaeger is an open-source distributed tracing system, and you can install it using pre-built binaries, Docker, or through package managers. This guide will cover the installation process using pre-built binaries. Step 1: Download the Jaeger Binary Go to the Jaeger GitHub releases page and find version 1.42.0. Or use wget to directly download the binary: wget https://github.com/jaegertracing/jaeger/releases/download/v1.42.0/jaeger-linux-amd64.tar.gz Once downloaded, extract the .tar.gz file: tar -xvzf jaeger-linux-amd64.tar.gz This will create a folder containing the Jaeger binaries (jaeger-agent, jaeger-collector, jaeger-query, etc.). Step 2: Move Jaeger to a System Directory Move the extracted binaries to a directory in your PATH (e.g., /usr/local/bin): mv jaeger-linux-amd64 /usr/local/bin/jaeger Ensure the binaries are executable: chmod +x /usr/local/bin/jaeger Step 3: Create a Configuration File (Optional) Create a directory for Jaeger configuration and add the jaeger-config.yaml file: mkdir -p /etc/jaeger vim /etc/jaeger/jaeger-config.yaml The content for jaeger-config.yaml: collector: http-port: 5775 grpc-port: 14250 agent: http-port: 5778 query: http-port: 16686 storage: type: cassandra cassandra: contact-points: - "localhost:9042" keyspace: jaeger timeout: 30s Step 4: Configure Data Directories Define storage directories for Jaeger: mkdir -p /var/jaeger/cassandra /var/jaeger/storage chown -R jaeger:jaeger /var/jaeger Create the Jaeger systemd service file: vim /etc/systemd/system/jaeger.service Add the following content: [Unit] Description=Jaeger Documentation=https://www.jaegertracing.io/docs/latest/ After=network.target [Service] ExecStart=/usr/local/bin/jaeger --config.file=/etc/jaeger/jaeger-config.yaml Restart=on-failure User=delian Group=delian [Install] WantedBy=multi-user.target Step 5: Start Jaeger Service Reload systemd, enable and start the Jaeger service: systemctl daemon-reload systemctl enable jaeger systemctl start jaeger systemctl status jaeger To monitor logs: journalctl -u jaeger -f Verify the Jaeger version: jaeger --version Conclusion By following the above steps, you should have Jaeger installed and running with the necessary configurations for distributed tracing. You can now proceed to integrate Jaeger with other components like Prometheus, Grafana, etc., for a complete monitoring and tracing solution.]]></summary></entry></feed>